{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000L, 3073L)\n",
      "Train labels shape:  (49000L,)\n",
      "Validation data shape:  (1000L, 3073L)\n",
      "Validation labels shape:  (1000L,)\n",
      "Test data shape:  (1000L, 3073L)\n",
      "Test labels shape:  (1000L,)\n",
      "dev data shape:  (500L, 3073L)\n",
      "dev labels shape:  (500L,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.316095\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.123063 analytic: -1.123063, relative error: 1.934031e-08\n",
      "numerical: -0.989914 analytic: -0.989914, relative error: 1.075238e-08\n",
      "numerical: -0.193250 analytic: -0.193250, relative error: 2.813657e-07\n",
      "numerical: -3.551652 analytic: -3.551652, relative error: 1.288338e-08\n",
      "numerical: 0.133002 analytic: 0.133002, relative error: 3.100369e-07\n",
      "numerical: -1.394305 analytic: -1.394305, relative error: 5.436599e-09\n",
      "numerical: -0.857374 analytic: -0.857374, relative error: 1.017384e-08\n",
      "numerical: 0.219674 analytic: 0.219674, relative error: 2.914641e-07\n",
      "numerical: 0.092425 analytic: 0.092425, relative error: 2.621390e-08\n",
      "numerical: 0.566136 analytic: 0.566136, relative error: 7.158479e-08\n",
      "numerical: 2.183977 analytic: 2.183977, relative error: 4.208127e-09\n",
      "numerical: 0.236028 analytic: 0.236028, relative error: 1.278750e-07\n",
      "numerical: -0.113968 analytic: -0.113968, relative error: 6.263921e-08\n",
      "numerical: -5.386152 analytic: -5.386152, relative error: 2.557038e-10\n",
      "numerical: 4.086001 analytic: 4.086000, relative error: 1.668488e-08\n",
      "numerical: 0.685026 analytic: 0.685026, relative error: 4.251349e-08\n",
      "numerical: -0.798038 analytic: -0.798038, relative error: 3.208353e-09\n",
      "numerical: 0.026952 analytic: 0.026952, relative error: 2.327318e-08\n",
      "numerical: 3.181331 analytic: 3.181331, relative error: 6.194709e-09\n",
      "numerical: -0.987672 analytic: -0.987672, relative error: 2.066936e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.316072e+00 computed in 0.142000s\n",
      "vectorized loss: 2.316072e+00 computed in 0.016000s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 500: loss 159.298207\n",
      "iteration 100 / 500: loss 129.382553\n",
      "iteration 200 / 500: loss 105.798336\n",
      "iteration 300 / 500: loss 86.726866\n",
      "iteration 400 / 500: loss 71.316289\n",
      "iteration 0 / 500: loss 158.820262\n",
      "iteration 100 / 500: loss 126.158248\n",
      "iteration 200 / 500: loss 100.409709\n",
      "iteration 300 / 500: loss 80.370785\n",
      "iteration 400 / 500: loss 64.317624\n",
      "iteration 0 / 500: loss 161.893628\n",
      "iteration 100 / 500: loss 124.132313\n",
      "iteration 200 / 500: loss 96.274965\n",
      "iteration 300 / 500: loss 75.193441\n",
      "iteration 400 / 500: loss 58.507891\n",
      "iteration 0 / 500: loss 160.196125\n",
      "iteration 100 / 500: loss 118.622525\n",
      "iteration 200 / 500: loss 89.068033\n",
      "iteration 300 / 500: loss 67.458088\n",
      "iteration 400 / 500: loss 50.865226\n",
      "iteration 0 / 500: loss 158.726922\n",
      "iteration 100 / 500: loss 114.507638\n",
      "iteration 200 / 500: loss 83.264174\n",
      "iteration 300 / 500: loss 60.623987\n",
      "iteration 400 / 500: loss 44.302498\n",
      "iteration 0 / 500: loss 157.997715\n",
      "iteration 100 / 500: loss 108.836446\n",
      "iteration 200 / 500: loss 76.110567\n",
      "iteration 300 / 500: loss 53.589749\n",
      "iteration 400 / 500: loss 37.743470\n",
      "iteration 0 / 500: loss 159.484045\n",
      "iteration 100 / 500: loss 105.622207\n",
      "iteration 200 / 500: loss 70.349237\n",
      "iteration 300 / 500: loss 47.581145\n",
      "iteration 400 / 500: loss 32.233618\n",
      "iteration 0 / 500: loss 160.579566\n",
      "iteration 100 / 500: loss 100.749696\n",
      "iteration 200 / 500: loss 63.842960\n",
      "iteration 300 / 500: loss 41.040802\n",
      "iteration 400 / 500: loss 26.534844\n",
      "iteration 0 / 500: loss 160.503972\n",
      "iteration 100 / 500: loss 94.401817\n",
      "iteration 200 / 500: loss 56.939760\n",
      "iteration 300 / 500: loss 34.533242\n",
      "iteration 400 / 500: loss 21.398656\n",
      "iteration 0 / 500: loss 160.857186\n",
      "iteration 100 / 500: loss 89.310240\n",
      "iteration 200 / 500: loss 50.452711\n",
      "iteration 300 / 500: loss 29.086023\n",
      "iteration 400 / 500: loss 16.948198\n",
      "iteration 0 / 500: loss 158.525812\n",
      "iteration 100 / 500: loss 82.067746\n",
      "iteration 200 / 500: loss 43.416232\n",
      "iteration 300 / 500: loss 23.457124\n",
      "iteration 400 / 500: loss 13.101385\n",
      "iteration 0 / 500: loss 159.909293\n",
      "iteration 100 / 500: loss 75.977032\n",
      "iteration 200 / 500: loss 37.225437\n",
      "iteration 300 / 500: loss 18.758700\n",
      "iteration 400 / 500: loss 9.940273\n",
      "iteration 0 / 500: loss 159.927713\n",
      "iteration 100 / 500: loss 69.259643\n",
      "iteration 200 / 500: loss 31.123057\n",
      "iteration 300 / 500: loss 14.654982\n",
      "iteration 400 / 500: loss 7.451611\n",
      "iteration 0 / 500: loss 158.657735\n",
      "iteration 100 / 500: loss 62.253719\n",
      "iteration 200 / 500: loss 25.630547\n",
      "iteration 300 / 500: loss 11.164700\n",
      "iteration 400 / 500: loss 5.651433\n",
      "iteration 0 / 500: loss 161.204212\n",
      "iteration 100 / 500: loss 56.464900\n",
      "iteration 200 / 500: loss 21.048817\n",
      "iteration 300 / 500: loss 8.529644\n",
      "iteration 400 / 500: loss 4.254788\n",
      "iteration 0 / 500: loss 157.265888\n",
      "iteration 100 / 500: loss 48.606234\n",
      "iteration 200 / 500: loss 16.170017\n",
      "iteration 300 / 500: loss 6.224552\n",
      "iteration 400 / 500: loss 3.278230\n",
      "iteration 0 / 500: loss 158.602419\n",
      "iteration 100 / 500: loss 42.384306\n",
      "iteration 200 / 500: loss 12.481440\n",
      "iteration 300 / 500: loss 4.686210\n",
      "iteration 400 / 500: loss 2.722717\n",
      "iteration 0 / 500: loss 156.730001\n",
      "iteration 100 / 500: loss 35.562370\n",
      "iteration 200 / 500: loss 9.398775\n",
      "iteration 300 / 500: loss 3.605359\n",
      "iteration 400 / 500: loss 2.328867\n",
      "iteration 0 / 500: loss 160.893176\n",
      "iteration 100 / 500: loss 30.535407\n",
      "iteration 200 / 500: loss 7.207346\n",
      "iteration 300 / 500: loss 2.941164\n",
      "iteration 400 / 500: loss 2.159298\n",
      "iteration 0 / 500: loss 160.073639\n",
      "iteration 100 / 500: loss 24.924421\n",
      "iteration 200 / 500: loss 5.406665\n",
      "iteration 300 / 500: loss 2.467372\n",
      "iteration 400 / 500: loss 1.940917\n",
      "iteration 0 / 500: loss 157.054972\n",
      "iteration 100 / 500: loss 19.764361\n",
      "iteration 200 / 500: loss 4.047321\n",
      "iteration 300 / 500: loss 2.229791\n",
      "iteration 400 / 500: loss 1.937512\n",
      "iteration 0 / 500: loss 158.042148\n",
      "iteration 100 / 500: loss 15.597789\n",
      "iteration 200 / 500: loss 3.129025\n",
      "iteration 300 / 500: loss 2.179210\n",
      "iteration 400 / 500: loss 1.998947\n",
      "iteration 0 / 500: loss 160.638938\n",
      "iteration 100 / 500: loss 12.139200\n",
      "iteration 200 / 500: loss 2.604719\n",
      "iteration 300 / 500: loss 2.003183\n",
      "iteration 400 / 500: loss 1.952601\n",
      "iteration 0 / 500: loss 156.335405\n",
      "iteration 100 / 500: loss 8.974054\n",
      "iteration 200 / 500: loss 2.278425\n",
      "iteration 300 / 500: loss 2.012794\n",
      "iteration 400 / 500: loss 1.949091\n",
      "iteration 0 / 500: loss 157.412489\n",
      "iteration 100 / 500: loss 6.798363\n",
      "iteration 200 / 500: loss 2.064355\n",
      "iteration 300 / 500: loss 1.837101\n",
      "iteration 400 / 500: loss 1.972369\n",
      "iteration 0 / 500: loss 158.573782\n",
      "iteration 100 / 500: loss 5.088362\n",
      "iteration 200 / 500: loss 1.978929\n",
      "iteration 300 / 500: loss 1.996868\n",
      "iteration 400 / 500: loss 2.005102\n",
      "iteration 0 / 500: loss 162.058035\n",
      "iteration 100 / 500: loss 3.875122\n",
      "iteration 200 / 500: loss 1.979980\n",
      "iteration 300 / 500: loss 1.874802\n",
      "iteration 400 / 500: loss 1.959605\n",
      "iteration 0 / 500: loss 159.401589\n",
      "iteration 100 / 500: loss 3.100446\n",
      "iteration 200 / 500: loss 1.971189\n",
      "iteration 300 / 500: loss 1.989507\n",
      "iteration 400 / 500: loss 1.945201\n",
      "iteration 0 / 500: loss 157.012252\n",
      "iteration 100 / 500: loss 2.565068\n",
      "iteration 200 / 500: loss 1.914697\n",
      "iteration 300 / 500: loss 1.941800\n",
      "iteration 400 / 500: loss 2.010982\n",
      "iteration 0 / 500: loss 159.653706\n",
      "iteration 100 / 500: loss 2.292134\n",
      "iteration 200 / 500: loss 1.983944\n",
      "iteration 300 / 500: loss 1.995531\n",
      "iteration 400 / 500: loss 2.015040\n",
      "iteration 0 / 500: loss 160.467077\n",
      "iteration 100 / 500: loss 2.134959\n",
      "iteration 200 / 500: loss 2.042412\n",
      "iteration 300 / 500: loss 2.091061\n",
      "iteration 400 / 500: loss 2.014612\n",
      "iteration 0 / 500: loss 158.612867\n",
      "iteration 100 / 500: loss 2.027723\n",
      "iteration 200 / 500: loss 2.036355\n",
      "iteration 300 / 500: loss 2.042748\n",
      "iteration 400 / 500: loss 2.030725\n",
      "iteration 0 / 500: loss 158.816575\n",
      "iteration 100 / 500: loss 2.063794\n",
      "iteration 200 / 500: loss 2.016861\n",
      "iteration 300 / 500: loss 2.257651\n",
      "iteration 400 / 500: loss 2.009383\n",
      "iteration 0 / 500: loss 155.970403\n",
      "iteration 100 / 500: loss 2.143568\n",
      "iteration 200 / 500: loss 2.179095\n",
      "iteration 300 / 500: loss 1.997536\n",
      "iteration 400 / 500: loss 2.017340\n",
      "iteration 0 / 500: loss 160.342359\n",
      "iteration 100 / 500: loss 2.476038\n",
      "iteration 200 / 500: loss 2.456874\n",
      "iteration 300 / 500: loss 2.053074\n",
      "iteration 400 / 500: loss 2.064930\n",
      "iteration 0 / 500: loss 155.947580\n",
      "iteration 100 / 500: loss 2.252815\n",
      "iteration 200 / 500: loss 2.401579\n",
      "iteration 300 / 500: loss 2.565723\n",
      "iteration 400 / 500: loss 2.439003\n",
      "iteration 0 / 500: loss 160.684866\n",
      "iteration 100 / 500: loss 3.064367\n",
      "iteration 200 / 500: loss 2.854015\n",
      "iteration 300 / 500: loss 3.310050\n",
      "iteration 400 / 500: loss 2.763347\n",
      "iteration 0 / 500: loss 158.031677\n",
      "iteration 100 / 500: loss 3.639286\n",
      "iteration 200 / 500: loss 3.307637\n",
      "iteration 300 / 500: loss 3.735048\n",
      "iteration 400 / 500: loss 2.991557\n",
      "iteration 0 / 500: loss 158.986149\n",
      "iteration 100 / 500: loss 3.210645\n",
      "iteration 200 / 500: loss 3.608713\n",
      "iteration 300 / 500: loss 3.044737\n",
      "iteration 400 / 500: loss 3.445082\n",
      "iteration 0 / 500: loss 158.194619\n",
      "iteration 100 / 500: loss 5.326600\n",
      "iteration 200 / 500: loss 3.416363\n",
      "iteration 300 / 500: loss 4.084315\n",
      "iteration 400 / 500: loss 3.727281\n",
      "iteration 0 / 500: loss 254.547686\n",
      "iteration 100 / 500: loss 182.620967\n",
      "iteration 200 / 500: loss 131.988548\n",
      "iteration 300 / 500: loss 95.615750\n",
      "iteration 400 / 500: loss 69.605434\n",
      "iteration 0 / 500: loss 257.051343\n",
      "iteration 100 / 500: loss 177.581010\n",
      "iteration 200 / 500: loss 123.318096\n",
      "iteration 300 / 500: loss 85.956894\n",
      "iteration 400 / 500: loss 60.065303\n",
      "iteration 0 / 500: loss 253.350457\n",
      "iteration 100 / 500: loss 167.703614\n",
      "iteration 200 / 500: loss 111.390077\n",
      "iteration 300 / 500: loss 74.127424\n",
      "iteration 400 / 500: loss 49.695201\n",
      "iteration 0 / 500: loss 252.803362\n",
      "iteration 100 / 500: loss 158.014997\n",
      "iteration 200 / 500: loss 99.942045\n",
      "iteration 300 / 500: loss 63.355838\n",
      "iteration 400 / 500: loss 40.546807\n",
      "iteration 0 / 500: loss 253.511435\n",
      "iteration 100 / 500: loss 149.426090\n",
      "iteration 200 / 500: loss 89.083365\n",
      "iteration 300 / 500: loss 53.481328\n",
      "iteration 400 / 500: loss 32.481721\n",
      "iteration 0 / 500: loss 253.636927\n",
      "iteration 100 / 500: loss 140.054299\n",
      "iteration 200 / 500: loss 78.287887\n",
      "iteration 300 / 500: loss 44.197936\n",
      "iteration 400 / 500: loss 25.408777\n",
      "iteration 0 / 500: loss 254.536191\n",
      "iteration 100 / 500: loss 130.373255\n",
      "iteration 200 / 500: loss 67.833611\n",
      "iteration 300 / 500: loss 35.882704\n",
      "iteration 400 / 500: loss 19.395302\n",
      "iteration 0 / 500: loss 254.969516\n",
      "iteration 100 / 500: loss 120.898534\n",
      "iteration 200 / 500: loss 58.117614\n",
      "iteration 300 / 500: loss 28.604042\n",
      "iteration 400 / 500: loss 14.575578\n",
      "iteration 0 / 500: loss 254.740034\n",
      "iteration 100 / 500: loss 109.839329\n",
      "iteration 200 / 500: loss 48.544569\n",
      "iteration 300 / 500: loss 22.013802\n",
      "iteration 400 / 500: loss 10.650356\n",
      "iteration 0 / 500: loss 254.103143\n",
      "iteration 100 / 500: loss 99.056052\n",
      "iteration 200 / 500: loss 39.596143\n",
      "iteration 300 / 500: loss 16.461590\n",
      "iteration 400 / 500: loss 7.659707\n",
      "iteration 0 / 500: loss 253.343482\n",
      "iteration 100 / 500: loss 87.846370\n",
      "iteration 200 / 500: loss 31.453063\n",
      "iteration 300 / 500: loss 12.049193\n",
      "iteration 400 / 500: loss 5.558551\n",
      "iteration 0 / 500: loss 255.447718\n",
      "iteration 100 / 500: loss 77.195492\n",
      "iteration 200 / 500: loss 24.647887\n",
      "iteration 300 / 500: loss 8.788825\n",
      "iteration 400 / 500: loss 4.031333\n",
      "iteration 0 / 500: loss 250.957568\n",
      "iteration 100 / 500: loss 65.964531\n",
      "iteration 200 / 500: loss 18.612391\n",
      "iteration 300 / 500: loss 6.263606\n",
      "iteration 400 / 500: loss 3.044055\n",
      "iteration 0 / 500: loss 255.117873\n",
      "iteration 100 / 500: loss 56.761515\n",
      "iteration 200 / 500: loss 13.878456\n",
      "iteration 300 / 500: loss 4.619787\n",
      "iteration 400 / 500: loss 2.507516\n",
      "iteration 0 / 500: loss 254.279770\n",
      "iteration 100 / 500: loss 47.026632\n",
      "iteration 200 / 500: loss 10.042365\n",
      "iteration 300 / 500: loss 3.450648\n",
      "iteration 400 / 500: loss 2.167186\n",
      "iteration 0 / 500: loss 254.927953\n",
      "iteration 100 / 500: loss 38.326078\n",
      "iteration 200 / 500: loss 7.332044\n",
      "iteration 300 / 500: loss 2.820456\n",
      "iteration 400 / 500: loss 2.097961\n",
      "iteration 0 / 500: loss 254.145130\n",
      "iteration 100 / 500: loss 30.434890\n",
      "iteration 200 / 500: loss 5.257375\n",
      "iteration 300 / 500: loss 2.314161\n",
      "iteration 400 / 500: loss 2.020043\n",
      "iteration 0 / 500: loss 256.550667\n",
      "iteration 100 / 500: loss 23.825061\n",
      "iteration 200 / 500: loss 3.823651\n",
      "iteration 300 / 500: loss 2.205519\n",
      "iteration 400 / 500: loss 1.981257\n",
      "iteration 0 / 500: loss 256.734830\n",
      "iteration 100 / 500: loss 18.060773\n",
      "iteration 200 / 500: loss 2.972257\n",
      "iteration 300 / 500: loss 2.068553\n",
      "iteration 400 / 500: loss 1.945076\n",
      "iteration 0 / 500: loss 253.490450\n",
      "iteration 100 / 500: loss 13.203576\n",
      "iteration 200 / 500: loss 2.422586\n",
      "iteration 300 / 500: loss 1.950093\n",
      "iteration 400 / 500: loss 2.025489\n",
      "iteration 0 / 500: loss 258.212128\n",
      "iteration 100 / 500: loss 9.663858\n",
      "iteration 200 / 500: loss 2.226995\n",
      "iteration 300 / 500: loss 2.021800\n",
      "iteration 400 / 500: loss 1.996630\n",
      "iteration 0 / 500: loss 253.810432\n",
      "iteration 100 / 500: loss 6.831673\n",
      "iteration 200 / 500: loss 2.116622\n",
      "iteration 300 / 500: loss 2.048573\n",
      "iteration 400 / 500: loss 2.002435\n",
      "iteration 0 / 500: loss 254.916339\n",
      "iteration 100 / 500: loss 4.941205\n",
      "iteration 200 / 500: loss 1.928244\n",
      "iteration 300 / 500: loss 1.968455\n",
      "iteration 400 / 500: loss 1.898938\n",
      "iteration 0 / 500: loss 254.667519\n",
      "iteration 100 / 500: loss 3.735519\n",
      "iteration 200 / 500: loss 2.036231\n",
      "iteration 300 / 500: loss 1.950252\n",
      "iteration 400 / 500: loss 2.014196\n",
      "iteration 0 / 500: loss 252.491140\n",
      "iteration 100 / 500: loss 2.937123\n",
      "iteration 200 / 500: loss 1.914931\n",
      "iteration 300 / 500: loss 2.034555\n",
      "iteration 400 / 500: loss 1.995067\n",
      "iteration 0 / 500: loss 252.595310\n",
      "iteration 100 / 500: loss 2.509257\n",
      "iteration 200 / 500: loss 1.974966\n",
      "iteration 300 / 500: loss 2.078470\n",
      "iteration 400 / 500: loss 1.999089\n",
      "iteration 0 / 500: loss 254.215475\n",
      "iteration 100 / 500: loss 2.205688\n",
      "iteration 200 / 500: loss 2.068575\n",
      "iteration 300 / 500: loss 2.038210\n",
      "iteration 400 / 500: loss 1.979663\n",
      "iteration 0 / 500: loss 255.721992\n",
      "iteration 100 / 500: loss 2.105233\n",
      "iteration 200 / 500: loss 2.005623\n",
      "iteration 300 / 500: loss 2.025796\n",
      "iteration 400 / 500: loss 2.002767\n",
      "iteration 0 / 500: loss 256.826592\n",
      "iteration 100 / 500: loss 2.054885\n",
      "iteration 200 / 500: loss 2.181471\n",
      "iteration 300 / 500: loss 2.018087\n",
      "iteration 400 / 500: loss 2.010722\n",
      "iteration 0 / 500: loss 254.724826\n",
      "iteration 100 / 500: loss 2.115784\n",
      "iteration 200 / 500: loss 2.017365\n",
      "iteration 300 / 500: loss 2.020142\n",
      "iteration 400 / 500: loss 2.181646\n",
      "iteration 0 / 500: loss 252.123408\n",
      "iteration 100 / 500: loss 1.983538\n",
      "iteration 200 / 500: loss 2.036491\n",
      "iteration 300 / 500: loss 2.039785\n",
      "iteration 400 / 500: loss 2.063214\n",
      "iteration 0 / 500: loss 256.541789\n",
      "iteration 100 / 500: loss 2.058035\n",
      "iteration 200 / 500: loss 2.257212\n",
      "iteration 300 / 500: loss 2.107889\n",
      "iteration 400 / 500: loss 2.167525\n",
      "iteration 0 / 500: loss 253.580235\n",
      "iteration 100 / 500: loss 2.057563\n",
      "iteration 200 / 500: loss 2.050997\n",
      "iteration 300 / 500: loss 2.094851\n",
      "iteration 400 / 500: loss 2.080995\n",
      "iteration 0 / 500: loss 255.686888\n",
      "iteration 100 / 500: loss 2.132867\n",
      "iteration 200 / 500: loss 2.148621\n",
      "iteration 300 / 500: loss 2.106627\n",
      "iteration 400 / 500: loss 2.348143\n",
      "iteration 0 / 500: loss 257.465551\n",
      "iteration 100 / 500: loss 2.627407\n",
      "iteration 200 / 500: loss 2.380062\n",
      "iteration 300 / 500: loss 2.390518\n",
      "iteration 400 / 500: loss 2.421660\n",
      "iteration 0 / 500: loss 257.315503\n",
      "iteration 100 / 500: loss 2.701658\n",
      "iteration 200 / 500: loss 2.737878\n",
      "iteration 300 / 500: loss 2.959869\n",
      "iteration 400 / 500: loss 2.590387\n",
      "iteration 0 / 500: loss 255.825893\n",
      "iteration 100 / 500: loss 2.823234\n",
      "iteration 200 / 500: loss 2.522042\n",
      "iteration 300 / 500: loss 3.562768\n",
      "iteration 400 / 500: loss 2.956782\n",
      "iteration 0 / 500: loss 253.785294\n",
      "iteration 100 / 500: loss 2.805281\n",
      "iteration 200 / 500: loss 3.284438\n",
      "iteration 300 / 500: loss 3.674165\n",
      "iteration 400 / 500: loss 3.797030\n",
      "iteration 0 / 500: loss 253.482681\n",
      "iteration 100 / 500: loss 3.729287\n",
      "iteration 200 / 500: loss 3.193891\n",
      "iteration 300 / 500: loss 4.570832\n",
      "iteration 400 / 500: loss 3.700550\n",
      "iteration 0 / 500: loss 257.011988\n",
      "iteration 100 / 500: loss 5.122109\n",
      "iteration 200 / 500: loss 4.382240\n",
      "iteration 300 / 500: loss 5.577029\n",
      "iteration 400 / 500: loss 4.796643\n",
      "iteration 0 / 500: loss 414.203917\n",
      "iteration 100 / 500: loss 243.788931\n",
      "iteration 200 / 500: loss 144.274005\n",
      "iteration 300 / 500: loss 85.723388\n",
      "iteration 400 / 500: loss 51.260131\n",
      "iteration 0 / 500: loss 416.202134\n",
      "iteration 100 / 500: loss 229.472910\n",
      "iteration 200 / 500: loss 126.860017\n",
      "iteration 300 / 500: loss 70.767601\n",
      "iteration 400 / 500: loss 39.862985\n",
      "iteration 0 / 500: loss 414.322577\n",
      "iteration 100 / 500: loss 211.792522\n",
      "iteration 200 / 500: loss 109.154803\n",
      "iteration 300 / 500: loss 56.743554\n",
      "iteration 400 / 500: loss 29.965700\n",
      "iteration 0 / 500: loss 406.080327\n",
      "iteration 100 / 500: loss 190.720610\n",
      "iteration 200 / 500: loss 90.548491\n",
      "iteration 300 / 500: loss 43.469757\n",
      "iteration 400 / 500: loss 21.563521\n",
      "iteration 0 / 500: loss 408.729404\n",
      "iteration 100 / 500: loss 174.954129\n",
      "iteration 200 / 500: loss 75.838819\n",
      "iteration 300 / 500: loss 33.520520\n",
      "iteration 400 / 500: loss 15.533977\n",
      "iteration 0 / 500: loss 410.995674\n",
      "iteration 100 / 500: loss 158.124876\n",
      "iteration 200 / 500: loss 62.020212\n",
      "iteration 300 / 500: loss 25.080439\n",
      "iteration 400 / 500: loss 10.947228\n",
      "iteration 0 / 500: loss 408.740495\n",
      "iteration 100 / 500: loss 139.847122\n",
      "iteration 200 / 500: loss 48.969046\n",
      "iteration 300 / 500: loss 17.949671\n",
      "iteration 400 / 500: loss 7.437177\n",
      "iteration 0 / 500: loss 406.237869\n",
      "iteration 100 / 500: loss 121.433740\n",
      "iteration 200 / 500: loss 37.525656\n",
      "iteration 300 / 500: loss 12.595189\n",
      "iteration 400 / 500: loss 5.169514\n",
      "iteration 0 / 500: loss 405.664516\n",
      "iteration 100 / 500: loss 104.328642\n",
      "iteration 200 / 500: loss 28.231182\n",
      "iteration 300 / 500: loss 8.695792\n",
      "iteration 400 / 500: loss 3.738024\n",
      "iteration 0 / 500: loss 408.475816\n",
      "iteration 100 / 500: loss 88.679388\n",
      "iteration 200 / 500: loss 20.650269\n",
      "iteration 300 / 500: loss 6.089695\n",
      "iteration 400 / 500: loss 2.885333\n",
      "iteration 0 / 500: loss 416.163537\n",
      "iteration 100 / 500: loss 74.943492\n",
      "iteration 200 / 500: loss 14.911170\n",
      "iteration 300 / 500: loss 4.284308\n",
      "iteration 400 / 500: loss 2.382012\n",
      "iteration 0 / 500: loss 409.414381\n",
      "iteration 100 / 500: loss 59.702614\n",
      "iteration 200 / 500: loss 10.210376\n",
      "iteration 300 / 500: loss 3.120795\n",
      "iteration 400 / 500: loss 2.144996\n",
      "iteration 0 / 500: loss 406.656239\n",
      "iteration 100 / 500: loss 46.734196\n",
      "iteration 200 / 500: loss 7.103164\n",
      "iteration 300 / 500: loss 2.550885\n",
      "iteration 400 / 500: loss 2.086698\n",
      "iteration 0 / 500: loss 413.062351\n",
      "iteration 100 / 500: loss 36.447522\n",
      "iteration 200 / 500: loss 4.926173\n",
      "iteration 300 / 500: loss 2.255658\n",
      "iteration 400 / 500: loss 2.077403\n",
      "iteration 0 / 500: loss 413.147192\n",
      "iteration 100 / 500: loss 27.192669\n",
      "iteration 200 / 500: loss 3.594138\n",
      "iteration 300 / 500: loss 2.142681\n",
      "iteration 400 / 500: loss 2.024118\n",
      "iteration 0 / 500: loss 412.462011\n",
      "iteration 100 / 500: loss 19.691028\n",
      "iteration 200 / 500: loss 2.760500\n",
      "iteration 300 / 500: loss 2.055179\n",
      "iteration 400 / 500: loss 2.012539\n",
      "iteration 0 / 500: loss 409.854382\n",
      "iteration 100 / 500: loss 13.896358\n",
      "iteration 200 / 500: loss 2.327728\n",
      "iteration 300 / 500: loss 2.090861\n",
      "iteration 400 / 500: loss 2.092990\n",
      "iteration 0 / 500: loss 412.054553\n",
      "iteration 100 / 500: loss 9.591304\n",
      "iteration 200 / 500: loss 2.203435\n",
      "iteration 300 / 500: loss 2.030110\n",
      "iteration 400 / 500: loss 2.026257\n",
      "iteration 0 / 500: loss 413.904020\n",
      "iteration 100 / 500: loss 6.646781\n",
      "iteration 200 / 500: loss 2.066636\n",
      "iteration 300 / 500: loss 2.062791\n",
      "iteration 400 / 500: loss 2.107800\n",
      "iteration 0 / 500: loss 409.975598\n",
      "iteration 100 / 500: loss 4.727709\n",
      "iteration 200 / 500: loss 2.077825\n",
      "iteration 300 / 500: loss 2.037555\n",
      "iteration 400 / 500: loss 2.037379\n",
      "iteration 0 / 500: loss 413.004845\n",
      "iteration 100 / 500: loss 3.427103\n",
      "iteration 200 / 500: loss 2.081364\n",
      "iteration 300 / 500: loss 1.993580\n",
      "iteration 400 / 500: loss 2.093447\n",
      "iteration 0 / 500: loss 403.909036\n",
      "iteration 100 / 500: loss 2.717132\n",
      "iteration 200 / 500: loss 2.044147\n",
      "iteration 300 / 500: loss 2.054465\n",
      "iteration 400 / 500: loss 2.106890\n",
      "iteration 0 / 500: loss 414.349160\n",
      "iteration 100 / 500: loss 2.385103\n",
      "iteration 200 / 500: loss 2.007742\n",
      "iteration 300 / 500: loss 2.070113\n",
      "iteration 400 / 500: loss 1.999550\n",
      "iteration 0 / 500: loss 407.938714\n",
      "iteration 100 / 500: loss 2.184043\n",
      "iteration 200 / 500: loss 1.969729\n",
      "iteration 300 / 500: loss 2.031623\n",
      "iteration 400 / 500: loss 2.000966\n",
      "iteration 0 / 500: loss 413.653172\n",
      "iteration 100 / 500: loss 2.100876\n",
      "iteration 200 / 500: loss 2.015518\n",
      "iteration 300 / 500: loss 1.983965\n",
      "iteration 400 / 500: loss 2.018194\n",
      "iteration 0 / 500: loss 411.540633\n",
      "iteration 100 / 500: loss 2.065133\n",
      "iteration 200 / 500: loss 2.030940\n",
      "iteration 300 / 500: loss 2.087031\n",
      "iteration 400 / 500: loss 2.032870\n",
      "iteration 0 / 500: loss 408.564936\n",
      "iteration 100 / 500: loss 2.015598\n",
      "iteration 200 / 500: loss 2.039506\n",
      "iteration 300 / 500: loss 2.092875\n",
      "iteration 400 / 500: loss 2.024163\n",
      "iteration 0 / 500: loss 411.314027\n",
      "iteration 100 / 500: loss 2.094192\n",
      "iteration 200 / 500: loss 1.994963\n",
      "iteration 300 / 500: loss 2.117844\n",
      "iteration 400 / 500: loss 2.134915\n",
      "iteration 0 / 500: loss 407.411175\n",
      "iteration 100 / 500: loss 2.063458\n",
      "iteration 200 / 500: loss 2.116316\n",
      "iteration 300 / 500: loss 2.054020\n",
      "iteration 400 / 500: loss 2.146367\n",
      "iteration 0 / 500: loss 414.242954\n",
      "iteration 100 / 500: loss 2.063221\n",
      "iteration 200 / 500: loss 2.091377\n",
      "iteration 300 / 500: loss 2.055259\n",
      "iteration 400 / 500: loss 2.128682\n",
      "iteration 0 / 500: loss 406.320388\n",
      "iteration 100 / 500: loss 2.072572\n",
      "iteration 200 / 500: loss 2.075190\n",
      "iteration 300 / 500: loss 2.110671\n",
      "iteration 400 / 500: loss 2.115012\n",
      "iteration 0 / 500: loss 415.136432\n",
      "iteration 100 / 500: loss 2.111115\n",
      "iteration 200 / 500: loss 2.067195\n",
      "iteration 300 / 500: loss 2.137479\n",
      "iteration 400 / 500: loss 2.231797\n",
      "iteration 0 / 500: loss 409.040225\n",
      "iteration 100 / 500: loss 2.023387\n",
      "iteration 200 / 500: loss 2.248659\n",
      "iteration 300 / 500: loss 2.149553\n",
      "iteration 400 / 500: loss 2.132552\n",
      "iteration 0 / 500: loss 410.823897\n",
      "iteration 100 / 500: loss 2.277704\n",
      "iteration 200 / 500: loss 2.262581\n",
      "iteration 300 / 500: loss 2.515012\n",
      "iteration 400 / 500: loss 2.184819\n",
      "iteration 0 / 500: loss 404.662712\n",
      "iteration 100 / 500: loss 2.782323\n",
      "iteration 200 / 500: loss 2.625143\n",
      "iteration 300 / 500: loss 2.227916\n",
      "iteration 400 / 500: loss 2.434865\n",
      "iteration 0 / 500: loss 412.243168\n",
      "iteration 100 / 500: loss 2.930830\n",
      "iteration 200 / 500: loss 3.482731\n",
      "iteration 300 / 500: loss 3.027189\n",
      "iteration 400 / 500: loss 2.506064\n",
      "iteration 0 / 500: loss 418.394813\n",
      "iteration 100 / 500: loss 3.733295\n",
      "iteration 200 / 500: loss 2.782670\n",
      "iteration 300 / 500: loss 3.928626\n",
      "iteration 400 / 500: loss 2.875581\n",
      "iteration 0 / 500: loss 411.346840\n",
      "iteration 100 / 500: loss 4.450264\n",
      "iteration 200 / 500: loss 4.041127\n",
      "iteration 300 / 500: loss 5.390618\n",
      "iteration 400 / 500: loss 4.637148\n",
      "iteration 0 / 500: loss 405.535552\n",
      "iteration 100 / 500: loss 3.565292\n",
      "iteration 200 / 500: loss 4.836878\n",
      "iteration 300 / 500: loss 6.089050\n",
      "iteration 400 / 500: loss 5.607156\n",
      "iteration 0 / 500: loss 408.842599\n",
      "iteration 100 / 500: loss 5.097377\n",
      "iteration 200 / 500: loss 5.468569\n",
      "iteration 300 / 500: loss 4.841796\n",
      "iteration 400 / 500: loss 3.995328\n",
      "iteration 0 / 500: loss 677.455812\n",
      "iteration 100 / 500: loss 286.604393\n",
      "iteration 200 / 500: loss 122.255745\n",
      "iteration 300 / 500: loss 52.909908\n",
      "iteration 400 / 500: loss 23.574600\n",
      "iteration 0 / 500: loss 660.838223\n",
      "iteration 100 / 500: loss 251.267027\n",
      "iteration 200 / 500: loss 96.847266\n",
      "iteration 300 / 500: loss 38.003429\n",
      "iteration 400 / 500: loss 15.635590\n",
      "iteration 0 / 500: loss 664.912190\n",
      "iteration 100 / 500: loss 224.203623\n",
      "iteration 200 / 500: loss 76.609702\n",
      "iteration 300 / 500: loss 27.120648\n",
      "iteration 400 / 500: loss 10.521700\n",
      "iteration 0 / 500: loss 663.788699\n",
      "iteration 100 / 500: loss 195.550457\n",
      "iteration 200 / 500: loss 58.626486\n",
      "iteration 300 / 500: loss 18.633865\n",
      "iteration 400 / 500: loss 6.951139\n",
      "iteration 0 / 500: loss 663.749267\n",
      "iteration 100 / 500: loss 167.464012\n",
      "iteration 200 / 500: loss 43.640689\n",
      "iteration 300 / 500: loss 12.528147\n",
      "iteration 400 / 500: loss 4.691962\n",
      "iteration 0 / 500: loss 671.344624\n",
      "iteration 100 / 500: loss 142.801373\n",
      "iteration 200 / 500: loss 31.879255\n",
      "iteration 300 / 500: loss 8.381072\n",
      "iteration 400 / 500: loss 3.394524\n",
      "iteration 0 / 500: loss 671.290777\n",
      "iteration 100 / 500: loss 117.868673\n",
      "iteration 200 / 500: loss 22.162184\n",
      "iteration 300 / 500: loss 5.579472\n",
      "iteration 400 / 500: loss 2.704627\n",
      "iteration 0 / 500: loss 656.125765\n",
      "iteration 100 / 500: loss 92.798938\n",
      "iteration 200 / 500: loss 14.703329\n",
      "iteration 300 / 500: loss 3.877340\n",
      "iteration 400 / 500: loss 2.304377\n",
      "iteration 0 / 500: loss 670.233788\n",
      "iteration 100 / 500: loss 74.434602\n",
      "iteration 200 / 500: loss 9.888198\n",
      "iteration 300 / 500: loss 2.929748\n",
      "iteration 400 / 500: loss 2.203836\n",
      "iteration 0 / 500: loss 667.184823\n",
      "iteration 100 / 500: loss 56.406213\n",
      "iteration 200 / 500: loss 6.547101\n",
      "iteration 300 / 500: loss 2.470159\n",
      "iteration 400 / 500: loss 2.115524\n",
      "iteration 0 / 500: loss 663.909919\n",
      "iteration 100 / 500: loss 41.573558\n",
      "iteration 200 / 500: loss 4.422853\n",
      "iteration 300 / 500: loss 2.264599\n",
      "iteration 400 / 500: loss 2.029858\n",
      "iteration 0 / 500: loss 669.867894\n",
      "iteration 100 / 500: loss 30.000255\n",
      "iteration 200 / 500: loss 3.160878\n",
      "iteration 300 / 500: loss 2.177837\n",
      "iteration 400 / 500: loss 2.045574\n",
      "iteration 0 / 500: loss 663.439200\n",
      "iteration 100 / 500: loss 20.645700\n",
      "iteration 200 / 500: loss 2.595944\n",
      "iteration 300 / 500: loss 2.060357\n",
      "iteration 400 / 500: loss 2.070097\n",
      "iteration 0 / 500: loss 667.309983\n",
      "iteration 100 / 500: loss 13.881648\n",
      "iteration 200 / 500: loss 2.320946\n",
      "iteration 300 / 500: loss 2.078520\n",
      "iteration 400 / 500: loss 2.013757\n",
      "iteration 0 / 500: loss 664.666765\n",
      "iteration 100 / 500: loss 9.157327\n",
      "iteration 200 / 500: loss 2.186454\n",
      "iteration 300 / 500: loss 2.144789\n",
      "iteration 400 / 500: loss 2.122954\n",
      "iteration 0 / 500: loss 664.919463\n",
      "iteration 100 / 500: loss 6.002795\n",
      "iteration 200 / 500: loss 2.069998\n",
      "iteration 300 / 500: loss 2.117418\n",
      "iteration 400 / 500: loss 2.061219\n",
      "iteration 0 / 500: loss 660.770870\n",
      "iteration 100 / 500: loss 4.230034\n",
      "iteration 200 / 500: loss 2.082352\n",
      "iteration 300 / 500: loss 2.103688\n",
      "iteration 400 / 500: loss 2.152164\n",
      "iteration 0 / 500: loss 665.900619\n",
      "iteration 100 / 500: loss 3.037917\n",
      "iteration 200 / 500: loss 2.096194\n",
      "iteration 300 / 500: loss 2.116489\n",
      "iteration 400 / 500: loss 2.061141\n",
      "iteration 0 / 500: loss 657.969206\n",
      "iteration 100 / 500: loss 2.445650\n",
      "iteration 200 / 500: loss 2.057668\n",
      "iteration 300 / 500: loss 2.059895\n",
      "iteration 400 / 500: loss 2.074588\n",
      "iteration 0 / 500: loss 668.738331\n",
      "iteration 100 / 500: loss 2.232175\n",
      "iteration 200 / 500: loss 2.079023\n",
      "iteration 300 / 500: loss 2.043266\n",
      "iteration 400 / 500: loss 2.121143\n",
      "iteration 0 / 500: loss 672.212374\n",
      "iteration 100 / 500: loss 2.136623\n",
      "iteration 200 / 500: loss 2.050227\n",
      "iteration 300 / 500: loss 2.085981\n",
      "iteration 400 / 500: loss 2.099581\n",
      "iteration 0 / 500: loss 662.915933\n",
      "iteration 100 / 500: loss 2.066013\n",
      "iteration 200 / 500: loss 2.030922\n",
      "iteration 300 / 500: loss 2.089613\n",
      "iteration 400 / 500: loss 2.080037\n",
      "iteration 0 / 500: loss 670.032916\n",
      "iteration 100 / 500: loss 2.113110\n",
      "iteration 200 / 500: loss 2.068748\n",
      "iteration 300 / 500: loss 2.082684\n",
      "iteration 400 / 500: loss 2.134507\n",
      "iteration 0 / 500: loss 669.391630\n",
      "iteration 100 / 500: loss 2.084549\n",
      "iteration 200 / 500: loss 2.086724\n",
      "iteration 300 / 500: loss 2.136496\n",
      "iteration 400 / 500: loss 2.107808\n",
      "iteration 0 / 500: loss 674.835323\n",
      "iteration 100 / 500: loss 2.055005\n",
      "iteration 200 / 500: loss 2.171891\n",
      "iteration 300 / 500: loss 2.060157\n",
      "iteration 400 / 500: loss 2.099664\n",
      "iteration 0 / 500: loss 662.465790\n",
      "iteration 100 / 500: loss 2.085747\n",
      "iteration 200 / 500: loss 2.115809\n",
      "iteration 300 / 500: loss 2.144094\n",
      "iteration 400 / 500: loss 2.146533\n",
      "iteration 0 / 500: loss 664.331046\n",
      "iteration 100 / 500: loss 2.150582\n",
      "iteration 200 / 500: loss 2.131676\n",
      "iteration 300 / 500: loss 2.185463\n",
      "iteration 400 / 500: loss 2.086104\n",
      "iteration 0 / 500: loss 655.731807\n",
      "iteration 100 / 500: loss 2.160915\n",
      "iteration 200 / 500: loss 2.090000\n",
      "iteration 300 / 500: loss 2.115461\n",
      "iteration 400 / 500: loss 2.152831\n",
      "iteration 0 / 500: loss 667.170684\n",
      "iteration 100 / 500: loss 2.134838\n",
      "iteration 200 / 500: loss 2.167313\n",
      "iteration 300 / 500: loss 2.080931\n",
      "iteration 400 / 500: loss 2.062805\n",
      "iteration 0 / 500: loss 663.140097\n",
      "iteration 100 / 500: loss 2.126942\n",
      "iteration 200 / 500: loss 2.121025\n",
      "iteration 300 / 500: loss 2.125078\n",
      "iteration 400 / 500: loss 2.122611\n",
      "iteration 0 / 500: loss 660.098986\n",
      "iteration 100 / 500: loss 2.197180\n",
      "iteration 200 / 500: loss 2.196529\n",
      "iteration 300 / 500: loss 2.135930\n",
      "iteration 400 / 500: loss 2.109426\n",
      "iteration 0 / 500: loss 660.636096\n",
      "iteration 100 / 500: loss 2.183777\n",
      "iteration 200 / 500: loss 2.172778\n",
      "iteration 300 / 500: loss 2.192719\n",
      "iteration 400 / 500: loss 2.200777\n",
      "iteration 0 / 500: loss 662.616165\n",
      "iteration 100 / 500: loss 2.158673\n",
      "iteration 200 / 500: loss 2.222510\n",
      "iteration 300 / 500: loss 2.358558\n",
      "iteration 400 / 500: loss 2.237367\n",
      "iteration 0 / 500: loss 666.531592\n",
      "iteration 100 / 500: loss 2.321870\n",
      "iteration 200 / 500: loss 2.328880\n",
      "iteration 300 / 500: loss 2.174388\n",
      "iteration 400 / 500: loss 2.421446\n",
      "iteration 0 / 500: loss 661.698856\n",
      "iteration 100 / 500: loss 2.667909\n",
      "iteration 200 / 500: loss 2.383430\n",
      "iteration 300 / 500: loss 2.996026\n",
      "iteration 400 / 500: loss 3.136992\n",
      "iteration 0 / 500: loss 662.071932\n",
      "iteration 100 / 500: loss 3.276214\n",
      "iteration 200 / 500: loss 3.637714\n",
      "iteration 300 / 500: loss 2.945324\n",
      "iteration 400 / 500: loss 2.718331\n",
      "iteration 0 / 500: loss 665.842761\n",
      "iteration 100 / 500: loss 3.303960\n",
      "iteration 200 / 500: loss 2.878686\n",
      "iteration 300 / 500: loss 3.375681\n",
      "iteration 400 / 500: loss 3.888270\n",
      "iteration 0 / 500: loss 662.159409\n",
      "iteration 100 / 500: loss 3.794228\n",
      "iteration 200 / 500: loss 4.550332\n",
      "iteration 300 / 500: loss 4.402186\n",
      "iteration 400 / 500: loss 5.749727\n",
      "iteration 0 / 500: loss 670.386263\n",
      "iteration 100 / 500: loss 4.460528\n",
      "iteration 200 / 500: loss 6.797329\n",
      "iteration 300 / 500: loss 4.830320\n",
      "iteration 400 / 500: loss 8.356470\n",
      "iteration 0 / 500: loss 660.794984\n",
      "iteration 100 / 500: loss 7.109917\n",
      "iteration 200 / 500: loss 8.092382\n",
      "iteration 300 / 500: loss 8.109096\n",
      "iteration 400 / 500: loss 4.749110\n",
      "iteration 0 / 500: loss 1062.406357\n",
      "iteration 100 / 500: loss 263.735793\n",
      "iteration 200 / 500: loss 66.735842\n",
      "iteration 300 / 500: loss 18.066258\n",
      "iteration 400 / 500: loss 6.063539\n",
      "iteration 0 / 500: loss 1076.210509\n",
      "iteration 100 / 500: loss 224.489514\n",
      "iteration 200 / 500: loss 48.207235\n",
      "iteration 300 / 500: loss 11.685962\n",
      "iteration 400 / 500: loss 4.103822\n",
      "iteration 0 / 500: loss 1069.686299\n",
      "iteration 100 / 500: loss 183.541038\n",
      "iteration 200 / 500: loss 32.927346\n",
      "iteration 300 / 500: loss 7.373583\n",
      "iteration 400 / 500: loss 2.991271\n",
      "iteration 0 / 500: loss 1064.981878\n",
      "iteration 100 / 500: loss 146.579481\n",
      "iteration 200 / 500: loss 21.722190\n",
      "iteration 300 / 500: loss 4.818248\n",
      "iteration 400 / 500: loss 2.515158\n",
      "iteration 0 / 500: loss 1051.765722\n",
      "iteration 100 / 500: loss 112.919048\n",
      "iteration 200 / 500: loss 13.785316\n",
      "iteration 300 / 500: loss 3.383935\n",
      "iteration 400 / 500: loss 2.255449\n",
      "iteration 0 / 500: loss 1056.893449\n",
      "iteration 100 / 500: loss 85.961509\n",
      "iteration 200 / 500: loss 8.836666\n",
      "iteration 300 / 500: loss 2.669011\n",
      "iteration 400 / 500: loss 2.201288\n",
      "iteration 0 / 500: loss 1091.221854\n",
      "iteration 100 / 500: loss 65.104740\n",
      "iteration 200 / 500: loss 5.771954\n",
      "iteration 300 / 500: loss 2.319449\n",
      "iteration 400 / 500: loss 2.129250\n",
      "iteration 0 / 500: loss 1078.589768\n",
      "iteration 100 / 500: loss 45.528904\n",
      "iteration 200 / 500: loss 3.851539\n",
      "iteration 300 / 500: loss 2.229752\n",
      "iteration 400 / 500: loss 2.132567\n",
      "iteration 0 / 500: loss 1065.289967\n",
      "iteration 100 / 500: loss 30.679625\n",
      "iteration 200 / 500: loss 2.923174\n",
      "iteration 300 / 500: loss 2.036247\n",
      "iteration 400 / 500: loss 2.078604\n",
      "iteration 0 / 500: loss 1081.194429\n",
      "iteration 100 / 500: loss 20.410008\n",
      "iteration 200 / 500: loss 2.455740\n",
      "iteration 300 / 500: loss 2.103603\n",
      "iteration 400 / 500: loss 2.125578\n",
      "iteration 0 / 500: loss 1066.982426\n",
      "iteration 100 / 500: loss 12.936565\n",
      "iteration 200 / 500: loss 2.175567\n",
      "iteration 300 / 500: loss 2.139196\n",
      "iteration 400 / 500: loss 2.154857\n",
      "iteration 0 / 500: loss 1069.893400\n",
      "iteration 100 / 500: loss 8.163303\n",
      "iteration 200 / 500: loss 2.095813\n",
      "iteration 300 / 500: loss 2.090717\n",
      "iteration 400 / 500: loss 2.051396\n",
      "iteration 0 / 500: loss 1065.678397\n",
      "iteration 100 / 500: loss 5.198572\n",
      "iteration 200 / 500: loss 2.148758\n",
      "iteration 300 / 500: loss 2.098300\n",
      "iteration 400 / 500: loss 2.088463\n",
      "iteration 0 / 500: loss 1088.930297\n",
      "iteration 100 / 500: loss 3.664237\n",
      "iteration 200 / 500: loss 2.147150\n",
      "iteration 300 / 500: loss 2.155325\n",
      "iteration 400 / 500: loss 2.148459\n",
      "iteration 0 / 500: loss 1079.384325\n",
      "iteration 100 / 500: loss 2.739843\n",
      "iteration 200 / 500: loss 2.141074\n",
      "iteration 300 / 500: loss 2.084452\n",
      "iteration 400 / 500: loss 2.112393\n",
      "iteration 0 / 500: loss 1083.654730\n",
      "iteration 100 / 500: loss 2.338276\n",
      "iteration 200 / 500: loss 2.149788\n",
      "iteration 300 / 500: loss 2.179036\n",
      "iteration 400 / 500: loss 2.104777\n",
      "iteration 0 / 500: loss 1061.976212\n",
      "iteration 100 / 500: loss 2.218101\n",
      "iteration 200 / 500: loss 2.062072\n",
      "iteration 300 / 500: loss 2.097345\n",
      "iteration 400 / 500: loss 2.090657\n",
      "iteration 0 / 500: loss 1084.774917\n",
      "iteration 100 / 500: loss 2.156695\n",
      "iteration 200 / 500: loss 2.085122\n",
      "iteration 300 / 500: loss 2.097754\n",
      "iteration 400 / 500: loss 2.108514\n",
      "iteration 0 / 500: loss 1086.145311\n",
      "iteration 100 / 500: loss 2.080769\n",
      "iteration 200 / 500: loss 2.162678\n",
      "iteration 300 / 500: loss 2.148153\n",
      "iteration 400 / 500: loss 2.143336\n",
      "iteration 0 / 500: loss 1081.942819\n",
      "iteration 100 / 500: loss 2.166607\n",
      "iteration 200 / 500: loss 2.187755\n",
      "iteration 300 / 500: loss 2.128485\n",
      "iteration 400 / 500: loss 2.122866\n",
      "iteration 0 / 500: loss 1075.430002\n",
      "iteration 100 / 500: loss 2.170133\n",
      "iteration 200 / 500: loss 2.098466\n",
      "iteration 300 / 500: loss 2.198164\n",
      "iteration 400 / 500: loss 2.101286\n",
      "iteration 0 / 500: loss 1072.945129\n",
      "iteration 100 / 500: loss 2.110189\n",
      "iteration 200 / 500: loss 2.081225\n",
      "iteration 300 / 500: loss 2.110630\n",
      "iteration 400 / 500: loss 2.080389\n",
      "iteration 0 / 500: loss 1093.544676\n",
      "iteration 100 / 500: loss 2.069048\n",
      "iteration 200 / 500: loss 2.123397\n",
      "iteration 300 / 500: loss 2.109896\n",
      "iteration 400 / 500: loss 2.140853\n",
      "iteration 0 / 500: loss 1081.429963\n",
      "iteration 100 / 500: loss 2.126120\n",
      "iteration 200 / 500: loss 2.133828\n",
      "iteration 300 / 500: loss 2.211958\n",
      "iteration 400 / 500: loss 2.111797\n",
      "iteration 0 / 500: loss 1060.098173\n",
      "iteration 100 / 500: loss 2.185106\n",
      "iteration 200 / 500: loss 2.099771\n",
      "iteration 300 / 500: loss 2.156675\n",
      "iteration 400 / 500: loss 2.197529\n",
      "iteration 0 / 500: loss 1059.950049\n",
      "iteration 100 / 500: loss 2.171157\n",
      "iteration 200 / 500: loss 2.108868\n",
      "iteration 300 / 500: loss 2.133682\n",
      "iteration 400 / 500: loss 2.101829\n",
      "iteration 0 / 500: loss 1077.956301\n",
      "iteration 100 / 500: loss 2.094422\n",
      "iteration 200 / 500: loss 2.157711\n",
      "iteration 300 / 500: loss 2.131738\n",
      "iteration 400 / 500: loss 2.161945\n",
      "iteration 0 / 500: loss 1080.558593\n",
      "iteration 100 / 500: loss 2.146637\n",
      "iteration 200 / 500: loss 2.244875\n",
      "iteration 300 / 500: loss 2.102264\n",
      "iteration 400 / 500: loss 2.114054\n",
      "iteration 0 / 500: loss 1066.420506\n",
      "iteration 100 / 500: loss 2.263217\n",
      "iteration 200 / 500: loss 2.269185\n",
      "iteration 300 / 500: loss 2.228823\n",
      "iteration 400 / 500: loss 2.166400\n",
      "iteration 0 / 500: loss 1068.354117\n",
      "iteration 100 / 500: loss 2.147273\n",
      "iteration 200 / 500: loss 2.192712\n",
      "iteration 300 / 500: loss 2.179741\n",
      "iteration 400 / 500: loss 2.239659\n",
      "iteration 0 / 500: loss 1056.793831\n",
      "iteration 100 / 500: loss 2.188866\n",
      "iteration 200 / 500: loss 2.135829\n",
      "iteration 300 / 500: loss 2.228292\n",
      "iteration 400 / 500: loss 2.206069\n",
      "iteration 0 / 500: loss 1066.913139\n",
      "iteration 100 / 500: loss 2.470682\n",
      "iteration 200 / 500: loss 2.211656\n",
      "iteration 300 / 500: loss 2.336109\n",
      "iteration 400 / 500: loss 2.206153\n",
      "iteration 0 / 500: loss 1072.840824\n",
      "iteration 100 / 500: loss 2.280109\n",
      "iteration 200 / 500: loss 2.492029\n",
      "iteration 300 / 500: loss 2.518728\n",
      "iteration 400 / 500: loss 2.255172\n",
      "iteration 0 / 500: loss 1062.915152\n",
      "iteration 100 / 500: loss 2.969213\n",
      "iteration 200 / 500: loss 2.637151\n",
      "iteration 300 / 500: loss 2.536231\n",
      "iteration 400 / 500: loss 3.069154\n",
      "iteration 0 / 500: loss 1066.134280\n",
      "iteration 100 / 500: loss 3.083039\n",
      "iteration 200 / 500: loss 3.421978\n",
      "iteration 300 / 500: loss 3.874973\n",
      "iteration 400 / 500: loss 3.676357\n",
      "iteration 0 / 500: loss 1059.042077\n",
      "iteration 100 / 500: loss 4.806612\n",
      "iteration 200 / 500: loss 4.661257\n",
      "iteration 300 / 500: loss 6.070849\n",
      "iteration 400 / 500: loss 3.768210\n",
      "iteration 0 / 500: loss 1085.304090\n",
      "iteration 100 / 500: loss 4.714443\n",
      "iteration 200 / 500: loss 4.944438\n",
      "iteration 300 / 500: loss 4.536421\n",
      "iteration 400 / 500: loss 4.684105\n",
      "iteration 0 / 500: loss 1064.763256\n",
      "iteration 100 / 500: loss 5.398305\n",
      "iteration 200 / 500: loss 6.187962\n",
      "iteration 300 / 500: loss 6.337840\n",
      "iteration 400 / 500: loss 8.817993\n",
      "iteration 0 / 500: loss 1085.679036\n",
      "iteration 100 / 500: loss 6.704929\n",
      "iteration 200 / 500: loss 8.600414\n",
      "iteration 300 / 500: loss 8.769005\n",
      "iteration 400 / 500: loss 7.958619\n",
      "iteration 0 / 500: loss 1083.270361\n",
      "iteration 100 / 500: loss 8.928763\n",
      "iteration 200 / 500: loss 9.954882\n",
      "iteration 300 / 500: loss 7.810288\n",
      "iteration 400 / 500: loss 11.526053\n",
      "iteration 0 / 500: loss 1753.383209\n",
      "iteration 100 / 500: loss 182.371184\n",
      "iteration 200 / 500: loss 20.666319\n",
      "iteration 300 / 500: loss 4.072023\n",
      "iteration 400 / 500: loss 2.378373\n",
      "iteration 0 / 500: loss 1744.351774\n",
      "iteration 100 / 500: loss 136.679819\n",
      "iteration 200 / 500: loss 12.527320\n",
      "iteration 300 / 500: loss 2.973061\n",
      "iteration 400 / 500: loss 2.188272\n",
      "iteration 0 / 500: loss 1727.396998\n",
      "iteration 100 / 500: loss 98.505293\n",
      "iteration 200 / 500: loss 7.482762\n",
      "iteration 300 / 500: loss 2.436876\n",
      "iteration 400 / 500: loss 2.146943\n",
      "iteration 0 / 500: loss 1747.664870\n",
      "iteration 100 / 500: loss 69.923983\n",
      "iteration 200 / 500: loss 4.830361\n",
      "iteration 300 / 500: loss 2.274675\n",
      "iteration 400 / 500: loss 2.156988\n",
      "iteration 0 / 500: loss 1756.598552\n",
      "iteration 100 / 500: loss 47.252975\n",
      "iteration 200 / 500: loss 3.305765\n",
      "iteration 300 / 500: loss 2.184409\n",
      "iteration 400 / 500: loss 2.139214\n",
      "iteration 0 / 500: loss 1747.601967\n",
      "iteration 100 / 500: loss 30.365433\n",
      "iteration 200 / 500: loss 2.589293\n",
      "iteration 300 / 500: loss 2.179162\n",
      "iteration 400 / 500: loss 2.079694\n",
      "iteration 0 / 500: loss 1744.687984\n",
      "iteration 100 / 500: loss 18.868914\n",
      "iteration 200 / 500: loss 2.327706\n",
      "iteration 300 / 500: loss 2.157772\n",
      "iteration 400 / 500: loss 2.157964\n",
      "iteration 0 / 500: loss 1734.016401\n",
      "iteration 100 / 500: loss 11.376230\n",
      "iteration 200 / 500: loss 2.194480\n",
      "iteration 300 / 500: loss 2.152587\n",
      "iteration 400 / 500: loss 2.172820\n",
      "iteration 0 / 500: loss 1748.270136\n",
      "iteration 100 / 500: loss 6.918250\n",
      "iteration 200 / 500: loss 2.206445\n",
      "iteration 300 / 500: loss 2.141006\n",
      "iteration 400 / 500: loss 2.089875\n",
      "iteration 0 / 500: loss 1755.594529\n",
      "iteration 100 / 500: loss 4.367300\n",
      "iteration 200 / 500: loss 2.144184\n",
      "iteration 300 / 500: loss 2.187285\n",
      "iteration 400 / 500: loss 2.115034\n",
      "iteration 0 / 500: loss 1724.865233\n",
      "iteration 100 / 500: loss 3.120646\n",
      "iteration 200 / 500: loss 2.184073\n",
      "iteration 300 / 500: loss 2.160816\n",
      "iteration 400 / 500: loss 2.113443\n",
      "iteration 0 / 500: loss 1735.302266\n",
      "iteration 100 / 500: loss 2.525826\n",
      "iteration 200 / 500: loss 2.191300\n",
      "iteration 300 / 500: loss 2.175460\n",
      "iteration 400 / 500: loss 2.149956\n",
      "iteration 0 / 500: loss 1728.197834\n",
      "iteration 100 / 500: loss 2.308052\n",
      "iteration 200 / 500: loss 2.113574\n",
      "iteration 300 / 500: loss 2.146913\n",
      "iteration 400 / 500: loss 2.171208\n",
      "iteration 0 / 500: loss 1717.378241\n",
      "iteration 100 / 500: loss 2.157206\n",
      "iteration 200 / 500: loss 2.184256\n",
      "iteration 300 / 500: loss 2.171268\n",
      "iteration 400 / 500: loss 2.183089\n",
      "iteration 0 / 500: loss 1755.220408\n",
      "iteration 100 / 500: loss 2.234183\n",
      "iteration 200 / 500: loss 2.188589\n",
      "iteration 300 / 500: loss 2.163727\n",
      "iteration 400 / 500: loss 2.128995\n",
      "iteration 0 / 500: loss 1763.119183\n",
      "iteration 100 / 500: loss 2.134882\n",
      "iteration 200 / 500: loss 2.178776\n",
      "iteration 300 / 500: loss 2.212250\n",
      "iteration 400 / 500: loss 2.196152\n",
      "iteration 0 / 500: loss 1737.880560\n",
      "iteration 100 / 500: loss 2.153053\n",
      "iteration 200 / 500: loss 2.148894\n",
      "iteration 300 / 500: loss 2.162489\n",
      "iteration 400 / 500: loss 2.197336\n",
      "iteration 0 / 500: loss 1721.244639\n",
      "iteration 100 / 500: loss 2.181234\n",
      "iteration 200 / 500: loss 2.174388\n",
      "iteration 300 / 500: loss 2.156565\n",
      "iteration 400 / 500: loss 2.153616\n",
      "iteration 0 / 500: loss 1730.188096\n",
      "iteration 100 / 500: loss 2.103143\n",
      "iteration 200 / 500: loss 2.167680\n",
      "iteration 300 / 500: loss 2.174466\n",
      "iteration 400 / 500: loss 2.203376\n",
      "iteration 0 / 500: loss 1739.866883\n",
      "iteration 100 / 500: loss 2.191664\n",
      "iteration 200 / 500: loss 2.155001\n",
      "iteration 300 / 500: loss 2.174643\n",
      "iteration 400 / 500: loss 2.158089\n",
      "iteration 0 / 500: loss 1769.864477\n",
      "iteration 100 / 500: loss 2.155268\n",
      "iteration 200 / 500: loss 2.142319\n",
      "iteration 300 / 500: loss 2.195393\n",
      "iteration 400 / 500: loss 2.145398\n",
      "iteration 0 / 500: loss 1723.640312\n",
      "iteration 100 / 500: loss 2.147423\n",
      "iteration 200 / 500: loss 2.138330\n",
      "iteration 300 / 500: loss 2.186940\n",
      "iteration 400 / 500: loss 2.225587\n",
      "iteration 0 / 500: loss 1727.515265\n",
      "iteration 100 / 500: loss 2.225662\n",
      "iteration 200 / 500: loss 2.159064\n",
      "iteration 300 / 500: loss 2.153025\n",
      "iteration 400 / 500: loss 2.132356\n",
      "iteration 0 / 500: loss 1752.773691\n",
      "iteration 100 / 500: loss 2.162003\n",
      "iteration 200 / 500: loss 2.150685\n",
      "iteration 300 / 500: loss 2.188785\n",
      "iteration 400 / 500: loss 2.205466\n",
      "iteration 0 / 500: loss 1717.497281\n",
      "iteration 100 / 500: loss 2.127200\n",
      "iteration 200 / 500: loss 2.183321\n",
      "iteration 300 / 500: loss 2.168833\n",
      "iteration 400 / 500: loss 2.150077\n",
      "iteration 0 / 500: loss 1745.909646\n",
      "iteration 100 / 500: loss 2.190862\n",
      "iteration 200 / 500: loss 2.194318\n",
      "iteration 300 / 500: loss 2.152364\n",
      "iteration 400 / 500: loss 2.142292\n",
      "iteration 0 / 500: loss 1742.605812\n",
      "iteration 100 / 500: loss 2.197241\n",
      "iteration 200 / 500: loss 2.170732\n",
      "iteration 300 / 500: loss 2.239927\n",
      "iteration 400 / 500: loss 2.196916\n",
      "iteration 0 / 500: loss 1756.759305\n",
      "iteration 100 / 500: loss 2.213894\n",
      "iteration 200 / 500: loss 2.167581\n",
      "iteration 300 / 500: loss 2.207634\n",
      "iteration 400 / 500: loss 2.203496\n",
      "iteration 0 / 500: loss 1731.668973\n",
      "iteration 100 / 500: loss 2.178929\n",
      "iteration 200 / 500: loss 2.172697\n",
      "iteration 300 / 500: loss 2.246142\n",
      "iteration 400 / 500: loss 2.210068\n",
      "iteration 0 / 500: loss 1730.796850\n",
      "iteration 100 / 500: loss 2.202214\n",
      "iteration 200 / 500: loss 2.249375\n",
      "iteration 300 / 500: loss 2.213216\n",
      "iteration 400 / 500: loss 2.240420\n",
      "iteration 0 / 500: loss 1729.624465\n",
      "iteration 100 / 500: loss 2.341981\n",
      "iteration 200 / 500: loss 2.240473\n",
      "iteration 300 / 500: loss 2.251545\n",
      "iteration 400 / 500: loss 2.300528\n",
      "iteration 0 / 500: loss 1730.637614\n",
      "iteration 100 / 500: loss 2.438236\n",
      "iteration 200 / 500: loss 2.250082\n",
      "iteration 300 / 500: loss 2.408328\n",
      "iteration 400 / 500: loss 2.248122\n",
      "iteration 0 / 500: loss 1735.428835\n",
      "iteration 100 / 500: loss 2.467092\n",
      "iteration 200 / 500: loss 3.133089\n",
      "iteration 300 / 500: loss 2.819380\n",
      "iteration 400 / 500: loss 2.809467\n",
      "iteration 0 / 500: loss 1728.553487\n",
      "iteration 100 / 500: loss 3.434136\n",
      "iteration 200 / 500: loss 3.535450\n",
      "iteration 300 / 500: loss 3.399458\n",
      "iteration 400 / 500: loss 3.218938\n",
      "iteration 0 / 500: loss 1716.860404\n",
      "iteration 100 / 500: loss 5.191258\n",
      "iteration 200 / 500: loss 4.759810\n",
      "iteration 300 / 500: loss 4.160919\n",
      "iteration 400 / 500: loss 4.289281\n",
      "iteration 0 / 500: loss 1753.459890\n",
      "iteration 100 / 500: loss 5.692355\n",
      "iteration 200 / 500: loss 6.879639\n",
      "iteration 300 / 500: loss 6.108435\n",
      "iteration 400 / 500: loss 5.779245\n",
      "iteration 0 / 500: loss 1706.197493\n",
      "iteration 100 / 500: loss 9.759701\n",
      "iteration 200 / 500: loss 8.693629\n",
      "iteration 300 / 500: loss 7.508406\n",
      "iteration 400 / 500: loss 5.796145\n",
      "iteration 0 / 500: loss 1750.658055\n",
      "iteration 100 / 500: loss 11.102640\n",
      "iteration 200 / 500: loss 11.268831\n",
      "iteration 300 / 500: loss 10.681587\n",
      "iteration 400 / 500: loss 12.153522\n",
      "iteration 0 / 500: loss 1731.771414\n",
      "iteration 100 / 500: loss 13.069331\n",
      "iteration 200 / 500: loss 11.951722\n",
      "iteration 300 / 500: loss 13.914108\n",
      "iteration 400 / 500: loss 10.351111\n",
      "iteration 0 / 500: loss 1720.711187\n",
      "iteration 100 / 500: loss 21.392829\n",
      "iteration 200 / 500: loss 21.493618\n",
      "iteration 300 / 500: loss 20.651014\n",
      "iteration 400 / 500: loss 23.268461\n",
      "iteration 0 / 500: loss 2836.546815\n",
      "iteration 100 / 500: loss 71.986162\n",
      "iteration 200 / 500: loss 3.908315\n",
      "iteration 300 / 500: loss 2.231173\n",
      "iteration 400 / 500: loss 2.178986\n",
      "iteration 0 / 500: loss 2804.659672\n",
      "iteration 100 / 500: loss 45.335281\n",
      "iteration 200 / 500: loss 2.866382\n",
      "iteration 300 / 500: loss 2.194936\n",
      "iteration 400 / 500: loss 2.194659\n",
      "iteration 0 / 500: loss 2788.625100\n",
      "iteration 100 / 500: loss 27.519242\n",
      "iteration 200 / 500: loss 2.393715\n",
      "iteration 300 / 500: loss 2.192090\n",
      "iteration 400 / 500: loss 2.237088\n",
      "iteration 0 / 500: loss 2835.201293\n",
      "iteration 100 / 500: loss 16.370400\n",
      "iteration 200 / 500: loss 2.205057\n",
      "iteration 300 / 500: loss 2.183620\n",
      "iteration 400 / 500: loss 2.184519\n",
      "iteration 0 / 500: loss 2831.933287\n",
      "iteration 100 / 500: loss 9.342122\n",
      "iteration 200 / 500: loss 2.194023\n",
      "iteration 300 / 500: loss 2.206497\n",
      "iteration 400 / 500: loss 2.191981\n",
      "iteration 0 / 500: loss 2856.899498\n",
      "iteration 100 / 500: loss 5.584140\n",
      "iteration 200 / 500: loss 2.201182\n",
      "iteration 300 / 500: loss 2.203242\n",
      "iteration 400 / 500: loss 2.220592\n",
      "iteration 0 / 500: loss 2790.571103\n",
      "iteration 100 / 500: loss 3.594066\n",
      "iteration 200 / 500: loss 2.144349\n",
      "iteration 300 / 500: loss 2.210126\n",
      "iteration 400 / 500: loss 2.198381\n",
      "iteration 0 / 500: loss 2844.801990\n",
      "iteration 100 / 500: loss 2.729825\n",
      "iteration 200 / 500: loss 2.219772\n",
      "iteration 300 / 500: loss 2.177941\n",
      "iteration 400 / 500: loss 2.225414\n",
      "iteration 0 / 500: loss 2848.087504\n",
      "iteration 100 / 500: loss 2.392922\n",
      "iteration 200 / 500: loss 2.202016\n",
      "iteration 300 / 500: loss 2.173094\n",
      "iteration 400 / 500: loss 2.196614\n",
      "iteration 0 / 500: loss 2823.836392\n",
      "iteration 100 / 500: loss 2.277014\n",
      "iteration 200 / 500: loss 2.175090\n",
      "iteration 300 / 500: loss 2.162662\n",
      "iteration 400 / 500: loss 2.211716\n",
      "iteration 0 / 500: loss 2843.110976\n",
      "iteration 100 / 500: loss 2.185479\n",
      "iteration 200 / 500: loss 2.218240\n",
      "iteration 300 / 500: loss 2.175828\n",
      "iteration 400 / 500: loss 2.158989\n",
      "iteration 0 / 500: loss 2837.151327\n",
      "iteration 100 / 500: loss 2.193924\n",
      "iteration 200 / 500: loss 2.168390\n",
      "iteration 300 / 500: loss 2.200757\n",
      "iteration 400 / 500: loss 2.221297\n",
      "iteration 0 / 500: loss 2791.440660\n",
      "iteration 100 / 500: loss 2.211367\n",
      "iteration 200 / 500: loss 2.159331\n",
      "iteration 300 / 500: loss 2.203253\n",
      "iteration 400 / 500: loss 2.191775\n",
      "iteration 0 / 500: loss 2837.975946\n",
      "iteration 100 / 500: loss 2.218855\n",
      "iteration 200 / 500: loss 2.169238\n",
      "iteration 300 / 500: loss 2.167912\n",
      "iteration 400 / 500: loss 2.167711\n",
      "iteration 0 / 500: loss 2784.198486\n",
      "iteration 100 / 500: loss 2.167827\n",
      "iteration 200 / 500: loss 2.197606\n",
      "iteration 300 / 500: loss 2.200891\n",
      "iteration 400 / 500: loss 2.174642\n",
      "iteration 0 / 500: loss 2837.826276\n",
      "iteration 100 / 500: loss 2.186098\n",
      "iteration 200 / 500: loss 2.214780\n",
      "iteration 300 / 500: loss 2.182402\n",
      "iteration 400 / 500: loss 2.185414\n",
      "iteration 0 / 500: loss 2778.488067\n",
      "iteration 100 / 500: loss 2.208782\n",
      "iteration 200 / 500: loss 2.232695\n",
      "iteration 300 / 500: loss 2.242063\n",
      "iteration 400 / 500: loss 2.184942\n",
      "iteration 0 / 500: loss 2839.346849\n",
      "iteration 100 / 500: loss 2.201396\n",
      "iteration 200 / 500: loss 2.188591\n",
      "iteration 300 / 500: loss 2.196130\n",
      "iteration 400 / 500: loss 2.211146\n",
      "iteration 0 / 500: loss 2825.170930\n",
      "iteration 100 / 500: loss 2.175406\n",
      "iteration 200 / 500: loss 2.227051\n",
      "iteration 300 / 500: loss 2.184843\n",
      "iteration 400 / 500: loss 2.192840\n",
      "iteration 0 / 500: loss 2831.663014\n",
      "iteration 100 / 500: loss 2.184994\n",
      "iteration 200 / 500: loss 2.230529\n",
      "iteration 300 / 500: loss 2.185146\n",
      "iteration 400 / 500: loss 2.211943\n",
      "iteration 0 / 500: loss 2830.417160\n",
      "iteration 100 / 500: loss 2.211347\n",
      "iteration 200 / 500: loss 2.193361\n",
      "iteration 300 / 500: loss 2.220840\n",
      "iteration 400 / 500: loss 2.220626\n",
      "iteration 0 / 500: loss 2785.547558\n",
      "iteration 100 / 500: loss 2.178580\n",
      "iteration 200 / 500: loss 2.202456\n",
      "iteration 300 / 500: loss 2.209514\n",
      "iteration 400 / 500: loss 2.200661\n",
      "iteration 0 / 500: loss 2818.318881\n",
      "iteration 100 / 500: loss 2.194220\n",
      "iteration 200 / 500: loss 2.192897\n",
      "iteration 300 / 500: loss 2.185898\n",
      "iteration 400 / 500: loss 2.273312\n",
      "iteration 0 / 500: loss 2862.565010\n",
      "iteration 100 / 500: loss 2.245144\n",
      "iteration 200 / 500: loss 2.241063\n",
      "iteration 300 / 500: loss 2.189113\n",
      "iteration 400 / 500: loss 2.234884\n",
      "iteration 0 / 500: loss 2830.278415\n",
      "iteration 100 / 500: loss 2.230069\n",
      "iteration 200 / 500: loss 2.187072\n",
      "iteration 300 / 500: loss 2.154036\n",
      "iteration 400 / 500: loss 2.215488\n",
      "iteration 0 / 500: loss 2819.863000\n",
      "iteration 100 / 500: loss 2.242464\n",
      "iteration 200 / 500: loss 2.214850\n",
      "iteration 300 / 500: loss 2.193435\n",
      "iteration 400 / 500: loss 2.235047\n",
      "iteration 0 / 500: loss 2832.556095\n",
      "iteration 100 / 500: loss 2.244082\n",
      "iteration 200 / 500: loss 2.216309\n",
      "iteration 300 / 500: loss 2.234279\n",
      "iteration 400 / 500: loss 2.208810\n",
      "iteration 0 / 500: loss 2811.079041\n",
      "iteration 100 / 500: loss 2.281601\n",
      "iteration 200 / 500: loss 2.258062\n",
      "iteration 300 / 500: loss 2.271453\n",
      "iteration 400 / 500: loss 2.243122\n",
      "iteration 0 / 500: loss 2862.698018\n",
      "iteration 100 / 500: loss 2.262420\n",
      "iteration 200 / 500: loss 2.241050\n",
      "iteration 300 / 500: loss 2.267217\n",
      "iteration 400 / 500: loss 2.331382\n",
      "iteration 0 / 500: loss 2835.663808\n",
      "iteration 100 / 500: loss 2.323845\n",
      "iteration 200 / 500: loss 2.250599\n",
      "iteration 300 / 500: loss 2.284143\n",
      "iteration 400 / 500: loss 2.236237\n",
      "iteration 0 / 500: loss 2855.974286\n",
      "iteration 100 / 500: loss 2.538583\n",
      "iteration 200 / 500: loss 2.297082\n",
      "iteration 300 / 500: loss 2.564898\n",
      "iteration 400 / 500: loss 2.372578\n",
      "iteration 0 / 500: loss 2828.342843\n",
      "iteration 100 / 500: loss 3.133066\n",
      "iteration 200 / 500: loss 3.217615\n",
      "iteration 300 / 500: loss 3.686159\n",
      "iteration 400 / 500: loss 3.345259\n",
      "iteration 0 / 500: loss 2848.077832\n",
      "iteration 100 / 500: loss 4.732353\n",
      "iteration 200 / 500: loss 4.584728\n",
      "iteration 300 / 500: loss 5.349484\n",
      "iteration 400 / 500: loss 6.094685\n",
      "iteration 0 / 500: loss 2803.807540\n",
      "iteration 100 / 500: loss 5.731361\n",
      "iteration 200 / 500: loss 7.787804\n",
      "iteration 300 / 500: loss 6.357478\n",
      "iteration 400 / 500: loss 6.483389\n",
      "iteration 0 / 500: loss 2845.113658\n",
      "iteration 100 / 500: loss 9.134593\n",
      "iteration 200 / 500: loss 9.850370\n",
      "iteration 300 / 500: loss 9.150310\n",
      "iteration 400 / 500: loss 9.511794\n",
      "iteration 0 / 500: loss 2791.159242\n",
      "iteration 100 / 500: loss 12.569753\n",
      "iteration 200 / 500: loss 13.000555\n",
      "iteration 300 / 500: loss 13.492577\n",
      "iteration 400 / 500: loss 11.744280\n",
      "iteration 0 / 500: loss 2842.847697\n",
      "iteration 100 / 500: loss 22.530797\n",
      "iteration 200 / 500: loss 20.224924\n",
      "iteration 300 / 500: loss 20.018090\n",
      "iteration 400 / 500: loss 22.532736\n",
      "iteration 0 / 500: loss 2808.567432\n",
      "iteration 100 / 500: loss 36.252672\n",
      "iteration 200 / 500: loss 45.203631\n",
      "iteration 300 / 500: loss 35.918075\n",
      "iteration 400 / 500: loss 35.291567\n",
      "iteration 0 / 500: loss 2778.778900\n",
      "iteration 100 / 500: loss 88.771610\n",
      "iteration 200 / 500: loss 82.701285\n",
      "iteration 300 / 500: loss 92.080753\n",
      "iteration 400 / 500: loss 88.011948\n",
      "iteration 0 / 500: loss 2802.681474\n",
      "iteration 100 / 500: loss 439.821412\n",
      "iteration 200 / 500: loss 425.929151\n",
      "iteration 300 / 500: loss 451.915426\n",
      "iteration 400 / 500: loss 442.621100\n",
      "iteration 0 / 500: loss 4576.881491\n",
      "iteration 100 / 500: loss 13.016547\n",
      "iteration 200 / 500: loss 2.260856\n",
      "iteration 300 / 500: loss 2.231561\n",
      "iteration 400 / 500: loss 2.203216\n",
      "iteration 0 / 500: loss 4577.780038\n",
      "iteration 100 / 500: loss 7.238176\n",
      "iteration 200 / 500: loss 2.249428\n",
      "iteration 300 / 500: loss 2.214455\n",
      "iteration 400 / 500: loss 2.184332\n",
      "iteration 0 / 500: loss 4614.142247\n",
      "iteration 100 / 500: loss 4.326480\n",
      "iteration 200 / 500: loss 2.192522\n",
      "iteration 300 / 500: loss 2.212770\n",
      "iteration 400 / 500: loss 2.192003\n",
      "iteration 0 / 500: loss 4624.767220\n",
      "iteration 100 / 500: loss 3.027783\n",
      "iteration 200 / 500: loss 2.211448\n",
      "iteration 300 / 500: loss 2.234426\n",
      "iteration 400 / 500: loss 2.187928\n",
      "iteration 0 / 500: loss 4606.101110\n",
      "iteration 100 / 500: loss 2.476217\n",
      "iteration 200 / 500: loss 2.218314\n",
      "iteration 300 / 500: loss 2.207145\n",
      "iteration 400 / 500: loss 2.211922\n",
      "iteration 0 / 500: loss 4603.576933\n",
      "iteration 100 / 500: loss 2.325615\n",
      "iteration 200 / 500: loss 2.237076\n",
      "iteration 300 / 500: loss 2.182249\n",
      "iteration 400 / 500: loss 2.222116\n",
      "iteration 0 / 500: loss 4538.745640\n",
      "iteration 100 / 500: loss 2.254882\n",
      "iteration 200 / 500: loss 2.233815\n",
      "iteration 300 / 500: loss 2.215359\n",
      "iteration 400 / 500: loss 2.248868\n",
      "iteration 0 / 500: loss 4589.449989\n",
      "iteration 100 / 500: loss 2.212543\n",
      "iteration 200 / 500: loss 2.202127\n",
      "iteration 300 / 500: loss 2.199457\n",
      "iteration 400 / 500: loss 2.172790\n",
      "iteration 0 / 500: loss 4624.899880\n",
      "iteration 100 / 500: loss 2.248372\n",
      "iteration 200 / 500: loss 2.225687\n",
      "iteration 300 / 500: loss 2.245865\n",
      "iteration 400 / 500: loss 2.218392\n",
      "iteration 0 / 500: loss 4553.400019\n",
      "iteration 100 / 500: loss 2.228172\n",
      "iteration 200 / 500: loss 2.237428\n",
      "iteration 300 / 500: loss 2.218099\n",
      "iteration 400 / 500: loss 2.229936\n",
      "iteration 0 / 500: loss 4607.541482\n",
      "iteration 100 / 500: loss 2.184984\n",
      "iteration 200 / 500: loss 2.220942\n",
      "iteration 300 / 500: loss 2.233876\n",
      "iteration 400 / 500: loss 2.200529\n",
      "iteration 0 / 500: loss 4564.845138\n",
      "iteration 100 / 500: loss 2.219398\n",
      "iteration 200 / 500: loss 2.211529\n",
      "iteration 300 / 500: loss 2.224437\n",
      "iteration 400 / 500: loss 2.209542\n",
      "iteration 0 / 500: loss 4531.315137\n",
      "iteration 100 / 500: loss 2.202566\n",
      "iteration 200 / 500: loss 2.233819\n",
      "iteration 300 / 500: loss 2.233200\n",
      "iteration 400 / 500: loss 2.228595\n",
      "iteration 0 / 500: loss 4584.757915\n",
      "iteration 100 / 500: loss 2.218792\n",
      "iteration 200 / 500: loss 2.206974\n",
      "iteration 300 / 500: loss 2.227772\n",
      "iteration 400 / 500: loss 2.223176\n",
      "iteration 0 / 500: loss 4587.041696\n",
      "iteration 100 / 500: loss 2.221620\n",
      "iteration 200 / 500: loss 2.183404\n",
      "iteration 300 / 500: loss 2.234719\n",
      "iteration 400 / 500: loss 2.200309\n",
      "iteration 0 / 500: loss 4541.520305\n",
      "iteration 100 / 500: loss 2.210710\n",
      "iteration 200 / 500: loss 2.220231\n",
      "iteration 300 / 500: loss 2.239291\n",
      "iteration 400 / 500: loss 2.245521\n",
      "iteration 0 / 500: loss 4533.039830\n",
      "iteration 100 / 500: loss 2.247851\n",
      "iteration 200 / 500: loss 2.266729\n",
      "iteration 300 / 500: loss 2.228788\n",
      "iteration 400 / 500: loss 2.222218\n",
      "iteration 0 / 500: loss 4528.144096\n",
      "iteration 100 / 500: loss 2.231748\n",
      "iteration 200 / 500: loss 2.221678\n",
      "iteration 300 / 500: loss 2.244480\n",
      "iteration 400 / 500: loss 2.238967\n",
      "iteration 0 / 500: loss 4597.134458\n",
      "iteration 100 / 500: loss 2.246748\n",
      "iteration 200 / 500: loss 2.231728\n",
      "iteration 300 / 500: loss 2.195925\n",
      "iteration 400 / 500: loss 2.234982\n",
      "iteration 0 / 500: loss 4579.863676\n",
      "iteration 100 / 500: loss 2.201329\n",
      "iteration 200 / 500: loss 2.229402\n",
      "iteration 300 / 500: loss 2.263203\n",
      "iteration 400 / 500: loss 2.227124\n",
      "iteration 0 / 500: loss 4593.038720\n",
      "iteration 100 / 500: loss 2.229482\n",
      "iteration 200 / 500: loss 2.254811\n",
      "iteration 300 / 500: loss 2.228667\n",
      "iteration 400 / 500: loss 2.230918\n",
      "iteration 0 / 500: loss 4557.783394\n",
      "iteration 100 / 500: loss 2.208331\n",
      "iteration 200 / 500: loss 2.267420\n",
      "iteration 300 / 500: loss 2.227520\n",
      "iteration 400 / 500: loss 2.239250\n",
      "iteration 0 / 500: loss 4584.296202\n",
      "iteration 100 / 500: loss 2.257903\n",
      "iteration 200 / 500: loss 2.258213\n",
      "iteration 300 / 500: loss 2.273141\n",
      "iteration 400 / 500: loss 2.236416\n",
      "iteration 0 / 500: loss 4618.915811\n",
      "iteration 100 / 500: loss 2.256484\n",
      "iteration 200 / 500: loss 2.238545\n",
      "iteration 300 / 500: loss 2.264173\n",
      "iteration 400 / 500: loss 2.239653\n",
      "iteration 0 / 500: loss 4614.775283\n",
      "iteration 100 / 500: loss 2.239313\n",
      "iteration 200 / 500: loss 2.243409\n",
      "iteration 300 / 500: loss 2.272330\n",
      "iteration 400 / 500: loss 2.239848\n",
      "iteration 0 / 500: loss 4590.925910\n",
      "iteration 100 / 500: loss 2.267305\n",
      "iteration 200 / 500: loss 2.238121\n",
      "iteration 300 / 500: loss 2.254004\n",
      "iteration 400 / 500: loss 2.279022\n",
      "iteration 0 / 500: loss 4591.607804\n",
      "iteration 100 / 500: loss 2.257877\n",
      "iteration 200 / 500: loss 2.291018\n",
      "iteration 300 / 500: loss 2.287871\n",
      "iteration 400 / 500: loss 2.263478\n",
      "iteration 0 / 500: loss 4585.586532\n",
      "iteration 100 / 500: loss 2.358816\n",
      "iteration 200 / 500: loss 2.286567\n",
      "iteration 300 / 500: loss 2.297232\n",
      "iteration 400 / 500: loss 2.353600\n",
      "iteration 0 / 500: loss 4577.133714\n",
      "iteration 100 / 500: loss 2.322652\n",
      "iteration 200 / 500: loss 2.487421\n",
      "iteration 300 / 500: loss 2.455018\n",
      "iteration 400 / 500: loss 2.333169\n",
      "iteration 0 / 500: loss 4576.015077\n",
      "iteration 100 / 500: loss 3.177922\n",
      "iteration 200 / 500: loss 3.395517\n",
      "iteration 300 / 500: loss 2.875782\n",
      "iteration 400 / 500: loss 3.516110\n",
      "iteration 0 / 500: loss 4535.822961\n",
      "iteration 100 / 500: loss 5.251220\n",
      "iteration 200 / 500: loss 5.158364\n",
      "iteration 300 / 500: loss 4.722014\n",
      "iteration 400 / 500: loss 5.317913\n",
      "iteration 0 / 500: loss 4603.565429\n",
      "iteration 100 / 500: loss 8.909142\n",
      "iteration 200 / 500: loss 8.680399\n",
      "iteration 300 / 500: loss 8.478330\n",
      "iteration 400 / 500: loss 8.412043\n",
      "iteration 0 / 500: loss 4647.007964\n",
      "iteration 100 / 500: loss 13.348779\n",
      "iteration 200 / 500: loss 13.154666\n",
      "iteration 300 / 500: loss 12.851575\n",
      "iteration 400 / 500: loss 13.916191\n",
      "iteration 0 / 500: loss 4594.335907\n",
      "iteration 100 / 500: loss 24.783924\n",
      "iteration 200 / 500: loss 24.647343\n",
      "iteration 300 / 500: loss 24.997480\n",
      "iteration 400 / 500: loss 24.131360\n",
      "iteration 0 / 500: loss 4633.432936\n",
      "iteration 100 / 500: loss 53.389739\n",
      "iteration 200 / 500: loss 58.387615\n",
      "iteration 300 / 500: loss 62.653430\n",
      "iteration 400 / 500: loss 63.824445\n",
      "iteration 0 / 500: loss 4563.158694\n",
      "iteration 100 / 500: loss 406.089693\n",
      "iteration 200 / 500: loss 362.522847\n",
      "iteration 300 / 500: loss 372.990255\n",
      "iteration 400 / 500: loss 397.052213\n",
      "iteration 0 / 500: loss 4587.762307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n\\classifiers\\softmax.py:91: RuntimeWarning: divide by zero encountered in log\n",
      "  probs_correct_log = -np.log(probs[xrange(num_train),y])\n",
      "cs231n\\classifiers\\softmax.py:90: RuntimeWarning: invalid value encountered in divide\n",
      "  probs             = scores_norm_exp / np.sum(scores_norm_exp, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 4557.327795\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 4609.462127\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 4532.946212\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7403.719248\n",
      "iteration 100 / 500: loss 2.607515\n",
      "iteration 200 / 500: loss 2.221130\n",
      "iteration 300 / 500: loss 2.232858\n",
      "iteration 400 / 500: loss 2.251185\n",
      "iteration 0 / 500: loss 7459.342992\n",
      "iteration 100 / 500: loss 2.337621\n",
      "iteration 200 / 500: loss 2.229377\n",
      "iteration 300 / 500: loss 2.235373\n",
      "iteration 400 / 500: loss 2.249862\n",
      "iteration 0 / 500: loss 7484.502102\n",
      "iteration 100 / 500: loss 2.283310\n",
      "iteration 200 / 500: loss 2.238913\n",
      "iteration 300 / 500: loss 2.234467\n",
      "iteration 400 / 500: loss 2.240874\n",
      "iteration 0 / 500: loss 7421.161868\n",
      "iteration 100 / 500: loss 2.248441\n",
      "iteration 200 / 500: loss 2.252895\n",
      "iteration 300 / 500: loss 2.223753\n",
      "iteration 400 / 500: loss 2.230965\n",
      "iteration 0 / 500: loss 7527.349126\n",
      "iteration 100 / 500: loss 2.224479\n",
      "iteration 200 / 500: loss 2.236276\n",
      "iteration 300 / 500: loss 2.239779\n",
      "iteration 400 / 500: loss 2.250169\n",
      "iteration 0 / 500: loss 7428.504229\n",
      "iteration 100 / 500: loss 2.258531\n",
      "iteration 200 / 500: loss 2.250856\n",
      "iteration 300 / 500: loss 2.237687\n",
      "iteration 400 / 500: loss 2.240411\n",
      "iteration 0 / 500: loss 7418.142587\n",
      "iteration 100 / 500: loss 2.263536\n",
      "iteration 200 / 500: loss 2.228386\n",
      "iteration 300 / 500: loss 2.244120\n",
      "iteration 400 / 500: loss 2.241776\n",
      "iteration 0 / 500: loss 7420.934753\n",
      "iteration 100 / 500: loss 2.251846\n",
      "iteration 200 / 500: loss 2.208869\n",
      "iteration 300 / 500: loss 2.229686\n",
      "iteration 400 / 500: loss 2.251458\n",
      "iteration 0 / 500: loss 7371.789556\n",
      "iteration 100 / 500: loss 2.239188\n",
      "iteration 200 / 500: loss 2.245075\n",
      "iteration 300 / 500: loss 2.247101\n",
      "iteration 400 / 500: loss 2.231185\n",
      "iteration 0 / 500: loss 7496.608820\n",
      "iteration 100 / 500: loss 2.254874\n",
      "iteration 200 / 500: loss 2.248415\n",
      "iteration 300 / 500: loss 2.259801\n",
      "iteration 400 / 500: loss 2.218533\n",
      "iteration 0 / 500: loss 7437.277274\n",
      "iteration 100 / 500: loss 2.249067\n",
      "iteration 200 / 500: loss 2.245332\n",
      "iteration 300 / 500: loss 2.256265\n",
      "iteration 400 / 500: loss 2.271119\n",
      "iteration 0 / 500: loss 7474.414102\n",
      "iteration 100 / 500: loss 2.258045\n",
      "iteration 200 / 500: loss 2.265124\n",
      "iteration 300 / 500: loss 2.269861\n",
      "iteration 400 / 500: loss 2.235921\n",
      "iteration 0 / 500: loss 7455.795519\n",
      "iteration 100 / 500: loss 2.235418\n",
      "iteration 200 / 500: loss 2.241940\n",
      "iteration 300 / 500: loss 2.250434\n",
      "iteration 400 / 500: loss 2.241878\n",
      "iteration 0 / 500: loss 7393.144747\n",
      "iteration 100 / 500: loss 2.278625\n",
      "iteration 200 / 500: loss 2.269282\n",
      "iteration 300 / 500: loss 2.248457\n",
      "iteration 400 / 500: loss 2.252450\n",
      "iteration 0 / 500: loss 7409.730437\n",
      "iteration 100 / 500: loss 2.219649\n",
      "iteration 200 / 500: loss 2.269436\n",
      "iteration 300 / 500: loss 2.248835\n",
      "iteration 400 / 500: loss 2.263523\n",
      "iteration 0 / 500: loss 7451.731134\n",
      "iteration 100 / 500: loss 2.259775\n",
      "iteration 200 / 500: loss 2.230215\n",
      "iteration 300 / 500: loss 2.262445\n",
      "iteration 400 / 500: loss 2.225609\n",
      "iteration 0 / 500: loss 7460.684938\n",
      "iteration 100 / 500: loss 2.233740\n",
      "iteration 200 / 500: loss 2.245941\n",
      "iteration 300 / 500: loss 2.250492\n",
      "iteration 400 / 500: loss 2.269122\n",
      "iteration 0 / 500: loss 7323.913389\n",
      "iteration 100 / 500: loss 2.268229\n",
      "iteration 200 / 500: loss 2.253864\n",
      "iteration 300 / 500: loss 2.250845\n",
      "iteration 400 / 500: loss 2.263453\n",
      "iteration 0 / 500: loss 7377.392022\n",
      "iteration 100 / 500: loss 2.263274\n",
      "iteration 200 / 500: loss 2.245326\n",
      "iteration 300 / 500: loss 2.275472\n",
      "iteration 400 / 500: loss 2.256207\n",
      "iteration 0 / 500: loss 7379.616661\n",
      "iteration 100 / 500: loss 2.292216\n",
      "iteration 200 / 500: loss 2.272717\n",
      "iteration 300 / 500: loss 2.258665\n",
      "iteration 400 / 500: loss 2.246197\n",
      "iteration 0 / 500: loss 7403.593030\n",
      "iteration 100 / 500: loss 2.270220\n",
      "iteration 200 / 500: loss 2.263989\n",
      "iteration 300 / 500: loss 2.260176\n",
      "iteration 400 / 500: loss 2.243347\n",
      "iteration 0 / 500: loss 7383.533070\n",
      "iteration 100 / 500: loss 2.272081\n",
      "iteration 200 / 500: loss 2.277066\n",
      "iteration 300 / 500: loss 2.288274\n",
      "iteration 400 / 500: loss 2.253202\n",
      "iteration 0 / 500: loss 7496.539681\n",
      "iteration 100 / 500: loss 2.273131\n",
      "iteration 200 / 500: loss 2.279609\n",
      "iteration 300 / 500: loss 2.264484\n",
      "iteration 400 / 500: loss 2.264439\n",
      "iteration 0 / 500: loss 7301.641229\n",
      "iteration 100 / 500: loss 2.324427\n",
      "iteration 200 / 500: loss 2.328072\n",
      "iteration 300 / 500: loss 2.286129\n",
      "iteration 400 / 500: loss 2.267787\n",
      "iteration 0 / 500: loss 7410.755102\n",
      "iteration 100 / 500: loss 2.295185\n",
      "iteration 200 / 500: loss 2.293542\n",
      "iteration 300 / 500: loss 2.326870\n",
      "iteration 400 / 500: loss 2.295658\n",
      "iteration 0 / 500: loss 7354.110915\n",
      "iteration 100 / 500: loss 2.293397\n",
      "iteration 200 / 500: loss 2.314593\n",
      "iteration 300 / 500: loss 2.339175\n",
      "iteration 400 / 500: loss 2.314815\n",
      "iteration 0 / 500: loss 7380.930124\n",
      "iteration 100 / 500: loss 2.445081\n",
      "iteration 200 / 500: loss 2.340221\n",
      "iteration 300 / 500: loss 2.435133\n",
      "iteration 400 / 500: loss 2.385063\n",
      "iteration 0 / 500: loss 7375.500641\n",
      "iteration 100 / 500: loss 3.826227\n",
      "iteration 200 / 500: loss 4.269324\n",
      "iteration 300 / 500: loss 4.140707\n",
      "iteration 400 / 500: loss 4.760616\n",
      "iteration 0 / 500: loss 7448.456888\n",
      "iteration 100 / 500: loss 7.457908\n",
      "iteration 200 / 500: loss 7.916532\n",
      "iteration 300 / 500: loss 7.985790\n",
      "iteration 400 / 500: loss 7.704238\n",
      "iteration 0 / 500: loss 7444.359472\n",
      "iteration 100 / 500: loss 14.612909\n",
      "iteration 200 / 500: loss 16.559593\n",
      "iteration 300 / 500: loss 15.510622\n",
      "iteration 400 / 500: loss 15.054517\n",
      "iteration 0 / 500: loss 7514.795610\n",
      "iteration 100 / 500: loss 42.124274\n",
      "iteration 200 / 500: loss 44.403117\n",
      "iteration 300 / 500: loss 36.779168\n",
      "iteration 400 / 500: loss 43.722488\n",
      "iteration 0 / 500: loss 7450.998981\n",
      "iteration 100 / 500: loss 328.204190\n",
      "iteration 200 / 500: loss 329.115844\n",
      "iteration 300 / 500: loss 339.962906\n",
      "iteration 400 / 500: loss 329.467879\n",
      "iteration 0 / 500: loss 7374.805183\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7391.407584\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7425.954504\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7467.689296\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7444.442442\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7293.039316\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7334.299498\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 7483.393096\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12090.578064\n",
      "iteration 100 / 500: loss 2.276525\n",
      "iteration 200 / 500: loss 2.280146\n",
      "iteration 300 / 500: loss 2.250176\n",
      "iteration 400 / 500: loss 2.239122\n",
      "iteration 0 / 500: loss 12127.835138\n",
      "iteration 100 / 500: loss 2.251521\n",
      "iteration 200 / 500: loss 2.254440\n",
      "iteration 300 / 500: loss 2.258600\n",
      "iteration 400 / 500: loss 2.253128\n",
      "iteration 0 / 500: loss 12217.031939\n",
      "iteration 100 / 500: loss 2.274524\n",
      "iteration 200 / 500: loss 2.248792\n",
      "iteration 300 / 500: loss 2.265120\n",
      "iteration 400 / 500: loss 2.267496\n",
      "iteration 0 / 500: loss 12185.263133\n",
      "iteration 100 / 500: loss 2.272666\n",
      "iteration 200 / 500: loss 2.260316\n",
      "iteration 300 / 500: loss 2.253359\n",
      "iteration 400 / 500: loss 2.251872\n",
      "iteration 0 / 500: loss 11988.110385\n",
      "iteration 100 / 500: loss 2.272451\n",
      "iteration 200 / 500: loss 2.268497\n",
      "iteration 300 / 500: loss 2.271143\n",
      "iteration 400 / 500: loss 2.262466\n",
      "iteration 0 / 500: loss 12088.097795\n",
      "iteration 100 / 500: loss 2.250509\n",
      "iteration 200 / 500: loss 2.263761\n",
      "iteration 300 / 500: loss 2.249907\n",
      "iteration 400 / 500: loss 2.255135\n",
      "iteration 0 / 500: loss 12126.023508\n",
      "iteration 100 / 500: loss 2.270340\n",
      "iteration 200 / 500: loss 2.273794\n",
      "iteration 300 / 500: loss 2.271083\n",
      "iteration 400 / 500: loss 2.256635\n",
      "iteration 0 / 500: loss 11970.708449\n",
      "iteration 100 / 500: loss 2.274540\n",
      "iteration 200 / 500: loss 2.262033\n",
      "iteration 300 / 500: loss 2.275037\n",
      "iteration 400 / 500: loss 2.266870\n",
      "iteration 0 / 500: loss 12170.745228\n",
      "iteration 100 / 500: loss 2.250398\n",
      "iteration 200 / 500: loss 2.277658\n",
      "iteration 300 / 500: loss 2.275190\n",
      "iteration 400 / 500: loss 2.255279\n",
      "iteration 0 / 500: loss 12046.277437\n",
      "iteration 100 / 500: loss 2.263408\n",
      "iteration 200 / 500: loss 2.276206\n",
      "iteration 300 / 500: loss 2.277781\n",
      "iteration 400 / 500: loss 2.264929\n",
      "iteration 0 / 500: loss 12125.456671\n",
      "iteration 100 / 500: loss 2.252532\n",
      "iteration 200 / 500: loss 2.252020\n",
      "iteration 300 / 500: loss 2.266556\n",
      "iteration 400 / 500: loss 2.272312\n",
      "iteration 0 / 500: loss 12253.322418\n",
      "iteration 100 / 500: loss 2.259027\n",
      "iteration 200 / 500: loss 2.262103\n",
      "iteration 300 / 500: loss 2.282160\n",
      "iteration 400 / 500: loss 2.266774\n",
      "iteration 0 / 500: loss 12224.609606\n",
      "iteration 100 / 500: loss 2.255382\n",
      "iteration 200 / 500: loss 2.270513\n",
      "iteration 300 / 500: loss 2.276285\n",
      "iteration 400 / 500: loss 2.268738\n",
      "iteration 0 / 500: loss 12157.465229\n",
      "iteration 100 / 500: loss 2.254838\n",
      "iteration 200 / 500: loss 2.275962\n",
      "iteration 300 / 500: loss 2.284965\n",
      "iteration 400 / 500: loss 2.268415\n",
      "iteration 0 / 500: loss 11921.209212\n",
      "iteration 100 / 500: loss 2.252463\n",
      "iteration 200 / 500: loss 2.265880\n",
      "iteration 300 / 500: loss 2.262064\n",
      "iteration 400 / 500: loss 2.269493\n",
      "iteration 0 / 500: loss 12103.655992\n",
      "iteration 100 / 500: loss 2.259068\n",
      "iteration 200 / 500: loss 2.287106\n",
      "iteration 300 / 500: loss 2.278468\n",
      "iteration 400 / 500: loss 2.271778\n",
      "iteration 0 / 500: loss 12069.505585\n",
      "iteration 100 / 500: loss 2.274684\n",
      "iteration 200 / 500: loss 2.266708\n",
      "iteration 300 / 500: loss 2.264791\n",
      "iteration 400 / 500: loss 2.280171\n",
      "iteration 0 / 500: loss 12027.641651\n",
      "iteration 100 / 500: loss 2.281967\n",
      "iteration 200 / 500: loss 2.285825\n",
      "iteration 300 / 500: loss 2.284824\n",
      "iteration 400 / 500: loss 2.283618\n",
      "iteration 0 / 500: loss 12030.066250\n",
      "iteration 100 / 500: loss 2.290590\n",
      "iteration 200 / 500: loss 2.269309\n",
      "iteration 300 / 500: loss 2.276914\n",
      "iteration 400 / 500: loss 2.271151\n",
      "iteration 0 / 500: loss 11880.658755\n",
      "iteration 100 / 500: loss 2.287879\n",
      "iteration 200 / 500: loss 2.285264\n",
      "iteration 300 / 500: loss 2.278046\n",
      "iteration 400 / 500: loss 2.264685\n",
      "iteration 0 / 500: loss 11961.046428\n",
      "iteration 100 / 500: loss 2.305962\n",
      "iteration 200 / 500: loss 2.286255\n",
      "iteration 300 / 500: loss 2.294012\n",
      "iteration 400 / 500: loss 2.307407\n",
      "iteration 0 / 500: loss 11918.920799\n",
      "iteration 100 / 500: loss 2.281804\n",
      "iteration 200 / 500: loss 2.329475\n",
      "iteration 300 / 500: loss 2.292306\n",
      "iteration 400 / 500: loss 2.300367\n",
      "iteration 0 / 500: loss 12198.126182\n",
      "iteration 100 / 500: loss 2.354770\n",
      "iteration 200 / 500: loss 2.324431\n",
      "iteration 300 / 500: loss 2.297131\n",
      "iteration 400 / 500: loss 2.312467\n",
      "iteration 0 / 500: loss 12180.668852\n",
      "iteration 100 / 500: loss 2.333115\n",
      "iteration 200 / 500: loss 2.440660\n",
      "iteration 300 / 500: loss 2.337537\n",
      "iteration 400 / 500: loss 2.381273\n",
      "iteration 0 / 500: loss 11969.684070\n",
      "iteration 100 / 500: loss 3.461655\n",
      "iteration 200 / 500: loss 3.341294\n",
      "iteration 300 / 500: loss 3.795290\n",
      "iteration 400 / 500: loss 3.596603\n",
      "iteration 0 / 500: loss 12190.058808\n",
      "iteration 100 / 500: loss 9.544449\n",
      "iteration 200 / 500: loss 9.122010\n",
      "iteration 300 / 500: loss 9.309738\n",
      "iteration 400 / 500: loss 9.110017\n",
      "iteration 0 / 500: loss 12023.920301\n",
      "iteration 100 / 500: loss 27.701642\n",
      "iteration 200 / 500: loss 26.201447\n",
      "iteration 300 / 500: loss 28.570471\n",
      "iteration 400 / 500: loss 28.866205\n",
      "iteration 0 / 500: loss 12187.160544\n",
      "iteration 100 / 500: loss 318.554561\n",
      "iteration 200 / 500: loss 294.810520\n",
      "iteration 300 / 500: loss 316.390907\n",
      "iteration 400 / 500: loss 305.175621\n",
      "iteration 0 / 500: loss 12164.872862\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12105.619610\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12020.900279\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12064.720303\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12031.423038\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12076.958038\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 11936.758350\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12052.452514\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 11865.802769\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12048.790489\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12075.927744\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 12053.679860\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19834.879234\n",
      "iteration 100 / 500: loss 2.274987\n",
      "iteration 200 / 500: loss 2.292586\n",
      "iteration 300 / 500: loss 2.276707\n",
      "iteration 400 / 500: loss 2.282613\n",
      "iteration 0 / 500: loss 19579.963106\n",
      "iteration 100 / 500: loss 2.281398\n",
      "iteration 200 / 500: loss 2.285000\n",
      "iteration 300 / 500: loss 2.280625\n",
      "iteration 400 / 500: loss 2.280601\n",
      "iteration 0 / 500: loss 19660.171636\n",
      "iteration 100 / 500: loss 2.273355\n",
      "iteration 200 / 500: loss 2.278895\n",
      "iteration 300 / 500: loss 2.278344\n",
      "iteration 400 / 500: loss 2.277333\n",
      "iteration 0 / 500: loss 19221.156579\n",
      "iteration 100 / 500: loss 2.282251\n",
      "iteration 200 / 500: loss 2.279262\n",
      "iteration 300 / 500: loss 2.273600\n",
      "iteration 400 / 500: loss 2.265920\n",
      "iteration 0 / 500: loss 19560.652665\n",
      "iteration 100 / 500: loss 2.281688\n",
      "iteration 200 / 500: loss 2.268266\n",
      "iteration 300 / 500: loss 2.284143\n",
      "iteration 400 / 500: loss 2.276236\n",
      "iteration 0 / 500: loss 19379.435322\n",
      "iteration 100 / 500: loss 2.283948\n",
      "iteration 200 / 500: loss 2.284220\n",
      "iteration 300 / 500: loss 2.276910\n",
      "iteration 400 / 500: loss 2.293523\n",
      "iteration 0 / 500: loss 19699.985200\n",
      "iteration 100 / 500: loss 2.271174\n",
      "iteration 200 / 500: loss 2.271557\n",
      "iteration 300 / 500: loss 2.280869\n",
      "iteration 400 / 500: loss 2.288534\n",
      "iteration 0 / 500: loss 19548.333061\n",
      "iteration 100 / 500: loss 2.271846\n",
      "iteration 200 / 500: loss 2.283745\n",
      "iteration 300 / 500: loss 2.272218\n",
      "iteration 400 / 500: loss 2.287691\n",
      "iteration 0 / 500: loss 19608.896266\n",
      "iteration 100 / 500: loss 2.275905\n",
      "iteration 200 / 500: loss 2.274474\n",
      "iteration 300 / 500: loss 2.270144\n",
      "iteration 400 / 500: loss 2.277830\n",
      "iteration 0 / 500: loss 19512.665213\n",
      "iteration 100 / 500: loss 2.276123\n",
      "iteration 200 / 500: loss 2.288964\n",
      "iteration 300 / 500: loss 2.268303\n",
      "iteration 400 / 500: loss 2.275399\n",
      "iteration 0 / 500: loss 19837.891864\n",
      "iteration 100 / 500: loss 2.280330\n",
      "iteration 200 / 500: loss 2.291877\n",
      "iteration 300 / 500: loss 2.282648\n",
      "iteration 400 / 500: loss 2.285949\n",
      "iteration 0 / 500: loss 19545.271915\n",
      "iteration 100 / 500: loss 2.282764\n",
      "iteration 200 / 500: loss 2.265808\n",
      "iteration 300 / 500: loss 2.283254\n",
      "iteration 400 / 500: loss 2.281796\n",
      "iteration 0 / 500: loss 19352.515333\n",
      "iteration 100 / 500: loss 2.278915\n",
      "iteration 200 / 500: loss 2.284523\n",
      "iteration 300 / 500: loss 2.290058\n",
      "iteration 400 / 500: loss 2.295296\n",
      "iteration 0 / 500: loss 19523.979942\n",
      "iteration 100 / 500: loss 2.291896\n",
      "iteration 200 / 500: loss 2.284843\n",
      "iteration 300 / 500: loss 2.279445\n",
      "iteration 400 / 500: loss 2.283987\n",
      "iteration 0 / 500: loss 19397.515116\n",
      "iteration 100 / 500: loss 2.275957\n",
      "iteration 200 / 500: loss 2.277914\n",
      "iteration 300 / 500: loss 2.279987\n",
      "iteration 400 / 500: loss 2.283517\n",
      "iteration 0 / 500: loss 19617.098227\n",
      "iteration 100 / 500: loss 2.291153\n",
      "iteration 200 / 500: loss 2.279000\n",
      "iteration 300 / 500: loss 2.284440\n",
      "iteration 400 / 500: loss 2.298373\n",
      "iteration 0 / 500: loss 19729.528864\n",
      "iteration 100 / 500: loss 2.282215\n",
      "iteration 200 / 500: loss 2.295302\n",
      "iteration 300 / 500: loss 2.283028\n",
      "iteration 400 / 500: loss 2.283032\n",
      "iteration 0 / 500: loss 19320.664538\n",
      "iteration 100 / 500: loss 2.311925\n",
      "iteration 200 / 500: loss 2.293822\n",
      "iteration 300 / 500: loss 2.290193\n",
      "iteration 400 / 500: loss 2.299038\n",
      "iteration 0 / 500: loss 19762.587745\n",
      "iteration 100 / 500: loss 2.303250\n",
      "iteration 200 / 500: loss 2.296194\n",
      "iteration 300 / 500: loss 2.289799\n",
      "iteration 400 / 500: loss 2.305643\n",
      "iteration 0 / 500: loss 19465.350836\n",
      "iteration 100 / 500: loss 2.319857\n",
      "iteration 200 / 500: loss 2.310725\n",
      "iteration 300 / 500: loss 2.310761\n",
      "iteration 400 / 500: loss 2.307993\n",
      "iteration 0 / 500: loss 19603.315822\n",
      "iteration 100 / 500: loss 2.321271\n",
      "iteration 200 / 500: loss 2.335626\n",
      "iteration 300 / 500: loss 2.349397\n",
      "iteration 400 / 500: loss 2.366386\n",
      "iteration 0 / 500: loss 19713.177579\n",
      "iteration 100 / 500: loss 3.490741\n",
      "iteration 200 / 500: loss 4.383176\n",
      "iteration 300 / 500: loss 3.465561\n",
      "iteration 400 / 500: loss 2.834906\n",
      "iteration 0 / 500: loss 19241.229630\n",
      "iteration 100 / 500: loss 18.667842\n",
      "iteration 200 / 500: loss 17.478666\n",
      "iteration 300 / 500: loss 18.444506\n",
      "iteration 400 / 500: loss 20.017592\n",
      "iteration 0 / 500: loss 19534.225015\n",
      "iteration 100 / 500: loss 335.491212\n",
      "iteration 200 / 500: loss 330.119704\n",
      "iteration 300 / 500: loss 328.419257\n",
      "iteration 400 / 500: loss 343.681930\n",
      "iteration 0 / 500: loss 19746.820714\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19652.302607\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19735.126913\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19509.789566\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19409.303947\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19476.119168\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19857.324451\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19475.621171\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19459.992550\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19443.540893\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19790.674807\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19636.377132\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19903.509125\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19604.528203\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19614.166745\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 19625.486913\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31551.243106\n",
      "iteration 100 / 500: loss 2.286174\n",
      "iteration 200 / 500: loss 2.279766\n",
      "iteration 300 / 500: loss 2.288424\n",
      "iteration 400 / 500: loss 2.283809\n",
      "iteration 0 / 500: loss 31295.259605\n",
      "iteration 100 / 500: loss 2.287050\n",
      "iteration 200 / 500: loss 2.283750\n",
      "iteration 300 / 500: loss 2.284167\n",
      "iteration 400 / 500: loss 2.282554\n",
      "iteration 0 / 500: loss 32165.869209\n",
      "iteration 100 / 500: loss 2.284946\n",
      "iteration 200 / 500: loss 2.286503\n",
      "iteration 300 / 500: loss 2.289870\n",
      "iteration 400 / 500: loss 2.280607\n",
      "iteration 0 / 500: loss 31619.091724\n",
      "iteration 100 / 500: loss 2.289907\n",
      "iteration 200 / 500: loss 2.278193\n",
      "iteration 300 / 500: loss 2.283975\n",
      "iteration 400 / 500: loss 2.280754\n",
      "iteration 0 / 500: loss 32266.020224\n",
      "iteration 100 / 500: loss 2.289995\n",
      "iteration 200 / 500: loss 2.282323\n",
      "iteration 300 / 500: loss 2.286564\n",
      "iteration 400 / 500: loss 2.286942\n",
      "iteration 0 / 500: loss 32151.860652\n",
      "iteration 100 / 500: loss 2.286447\n",
      "iteration 200 / 500: loss 2.284054\n",
      "iteration 300 / 500: loss 2.293899\n",
      "iteration 400 / 500: loss 2.286783\n",
      "iteration 0 / 500: loss 31911.982669\n",
      "iteration 100 / 500: loss 2.284051\n",
      "iteration 200 / 500: loss 2.288901\n",
      "iteration 300 / 500: loss 2.288988\n",
      "iteration 400 / 500: loss 2.289063\n",
      "iteration 0 / 500: loss 32343.659283\n",
      "iteration 100 / 500: loss 2.295867\n",
      "iteration 200 / 500: loss 2.293881\n",
      "iteration 300 / 500: loss 2.289268\n",
      "iteration 400 / 500: loss 2.295588\n",
      "iteration 0 / 500: loss 32090.491306\n",
      "iteration 100 / 500: loss 2.292379\n",
      "iteration 200 / 500: loss 2.283201\n",
      "iteration 300 / 500: loss 2.282173\n",
      "iteration 400 / 500: loss 2.289383\n",
      "iteration 0 / 500: loss 31659.980001\n",
      "iteration 100 / 500: loss 2.287125\n",
      "iteration 200 / 500: loss 2.285612\n",
      "iteration 300 / 500: loss 2.297216\n",
      "iteration 400 / 500: loss 2.278443\n",
      "iteration 0 / 500: loss 31645.573510\n",
      "iteration 100 / 500: loss 2.292652\n",
      "iteration 200 / 500: loss 2.293631\n",
      "iteration 300 / 500: loss 2.292538\n",
      "iteration 400 / 500: loss 2.293181\n",
      "iteration 0 / 500: loss 32163.580870\n",
      "iteration 100 / 500: loss 2.294288\n",
      "iteration 200 / 500: loss 2.290838\n",
      "iteration 300 / 500: loss 2.287128\n",
      "iteration 400 / 500: loss 2.293142\n",
      "iteration 0 / 500: loss 31715.496762\n",
      "iteration 100 / 500: loss 2.296115\n",
      "iteration 200 / 500: loss 2.305530\n",
      "iteration 300 / 500: loss 2.290467\n",
      "iteration 400 / 500: loss 2.290098\n",
      "iteration 0 / 500: loss 31935.186290\n",
      "iteration 100 / 500: loss 2.293838\n",
      "iteration 200 / 500: loss 2.297418\n",
      "iteration 300 / 500: loss 2.289047\n",
      "iteration 400 / 500: loss 2.297812\n",
      "iteration 0 / 500: loss 32100.233508\n",
      "iteration 100 / 500: loss 2.306784\n",
      "iteration 200 / 500: loss 2.305924\n",
      "iteration 300 / 500: loss 2.300109\n",
      "iteration 400 / 500: loss 2.291087\n",
      "iteration 0 / 500: loss 31990.402086\n",
      "iteration 100 / 500: loss 2.302427\n",
      "iteration 200 / 500: loss 2.298934\n",
      "iteration 300 / 500: loss 2.306626\n",
      "iteration 400 / 500: loss 2.308578\n",
      "iteration 0 / 500: loss 31919.055820\n",
      "iteration 100 / 500: loss 2.304572\n",
      "iteration 200 / 500: loss 2.347877\n",
      "iteration 300 / 500: loss 2.318750\n",
      "iteration 400 / 500: loss 2.309329\n",
      "iteration 0 / 500: loss 32296.263723\n",
      "iteration 100 / 500: loss 2.344666\n",
      "iteration 200 / 500: loss 2.358839\n",
      "iteration 300 / 500: loss 2.347323\n",
      "iteration 400 / 500: loss 2.368010\n",
      "iteration 0 / 500: loss 31991.459322\n",
      "iteration 100 / 500: loss 10.219325\n",
      "iteration 200 / 500: loss 9.936499\n",
      "iteration 300 / 500: loss 9.247298\n",
      "iteration 400 / 500: loss 11.650831\n",
      "iteration 0 / 500: loss 31850.834827\n",
      "iteration 100 / 500: loss 431.201594\n",
      "iteration 200 / 500: loss 438.996740\n",
      "iteration 300 / 500: loss 456.495262\n",
      "iteration 400 / 500: loss 459.987606\n",
      "iteration 0 / 500: loss 31397.872100\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31886.183785\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31763.212797\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31913.045147\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 32083.080363\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 32052.048289\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 32172.652267\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 32232.361001\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31714.187902\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31822.200196\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31593.808895\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31885.124732\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31831.163423\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31780.374004\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31366.209373\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31679.484192\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31777.065098\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31684.419045\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 32074.518410\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 31527.514692\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51501.863356\n",
      "iteration 100 / 500: loss 2.292230\n",
      "iteration 200 / 500: loss 2.286804\n",
      "iteration 300 / 500: loss 2.293272\n",
      "iteration 400 / 500: loss 2.294143\n",
      "iteration 0 / 500: loss 52050.131418\n",
      "iteration 100 / 500: loss 2.295515\n",
      "iteration 200 / 500: loss 2.293962\n",
      "iteration 300 / 500: loss 2.294647\n",
      "iteration 400 / 500: loss 2.290873\n",
      "iteration 0 / 500: loss 52293.253700\n",
      "iteration 100 / 500: loss 2.294769\n",
      "iteration 200 / 500: loss 2.287698\n",
      "iteration 300 / 500: loss 2.293341\n",
      "iteration 400 / 500: loss 2.293848\n",
      "iteration 0 / 500: loss 51023.639658\n",
      "iteration 100 / 500: loss 2.295513\n",
      "iteration 200 / 500: loss 2.295303\n",
      "iteration 300 / 500: loss 2.291779\n",
      "iteration 400 / 500: loss 2.291392\n",
      "iteration 0 / 500: loss 51663.809713\n",
      "iteration 100 / 500: loss 2.297142\n",
      "iteration 200 / 500: loss 2.295348\n",
      "iteration 300 / 500: loss 2.294637\n",
      "iteration 400 / 500: loss 2.294294\n",
      "iteration 0 / 500: loss 51553.411947\n",
      "iteration 100 / 500: loss 2.292702\n",
      "iteration 200 / 500: loss 2.294583\n",
      "iteration 300 / 500: loss 2.293503\n",
      "iteration 400 / 500: loss 2.288459\n",
      "iteration 0 / 500: loss 51746.703354\n",
      "iteration 100 / 500: loss 2.293153\n",
      "iteration 200 / 500: loss 2.294766\n",
      "iteration 300 / 500: loss 2.295740\n",
      "iteration 400 / 500: loss 2.295584\n",
      "iteration 0 / 500: loss 51804.618952\n",
      "iteration 100 / 500: loss 2.293508\n",
      "iteration 200 / 500: loss 2.300285\n",
      "iteration 300 / 500: loss 2.297252\n",
      "iteration 400 / 500: loss 2.300395\n",
      "iteration 0 / 500: loss 51047.875515\n",
      "iteration 100 / 500: loss 2.296030\n",
      "iteration 200 / 500: loss 2.296036\n",
      "iteration 300 / 500: loss 2.295166\n",
      "iteration 400 / 500: loss 2.299348\n",
      "iteration 0 / 500: loss 51014.916378\n",
      "iteration 100 / 500: loss 2.299389\n",
      "iteration 200 / 500: loss 2.300044\n",
      "iteration 300 / 500: loss 2.294031\n",
      "iteration 400 / 500: loss 2.298944\n",
      "iteration 0 / 500: loss 51781.613196\n",
      "iteration 100 / 500: loss 2.298711\n",
      "iteration 200 / 500: loss 2.297342\n",
      "iteration 300 / 500: loss 2.295723\n",
      "iteration 400 / 500: loss 2.303737\n",
      "iteration 0 / 500: loss 51681.367410\n",
      "iteration 100 / 500: loss 2.313490\n",
      "iteration 200 / 500: loss 2.300722\n",
      "iteration 300 / 500: loss 2.303992\n",
      "iteration 400 / 500: loss 2.296967\n",
      "iteration 0 / 500: loss 51901.433580\n",
      "iteration 100 / 500: loss 2.316376\n",
      "iteration 200 / 500: loss 2.304679\n",
      "iteration 300 / 500: loss 2.320894\n",
      "iteration 400 / 500: loss 2.306721\n",
      "iteration 0 / 500: loss 51227.964612\n",
      "iteration 100 / 500: loss 2.320745\n",
      "iteration 200 / 500: loss 2.336528\n",
      "iteration 300 / 500: loss 2.317202\n",
      "iteration 400 / 500: loss 2.318830\n",
      "iteration 0 / 500: loss 52149.772665\n",
      "iteration 100 / 500: loss 2.430764\n",
      "iteration 200 / 500: loss 2.418742\n",
      "iteration 300 / 500: loss 2.350925\n",
      "iteration 400 / 500: loss 2.383447\n",
      "iteration 0 / 500: loss 51331.213102\n",
      "iteration 100 / 500: loss 1166.015074\n",
      "iteration 200 / 500: loss 1041.466170\n",
      "iteration 300 / 500: loss 1069.564937\n",
      "iteration 400 / 500: loss 1099.281633\n",
      "iteration 0 / 500: loss 51730.618654\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51593.170822\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51413.174531\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51204.002121\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51471.286558\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51544.705405\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51857.836507\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51722.515381\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51038.908045\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51751.455157\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 52113.292220\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 52179.525957\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51335.107754\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51589.383869\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 52353.588405\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 50933.125105\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51236.409634\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51922.965595\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 52175.498526\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51103.479716\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51686.386593\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 50658.526954\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51943.947127\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 51447.191902\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84019.843015\n",
      "iteration 100 / 500: loss 2.298443\n",
      "iteration 200 / 500: loss 2.298482\n",
      "iteration 300 / 500: loss 2.296807\n",
      "iteration 400 / 500: loss 2.296437\n",
      "iteration 0 / 500: loss 83473.055527\n",
      "iteration 100 / 500: loss 2.297476\n",
      "iteration 200 / 500: loss 2.297238\n",
      "iteration 300 / 500: loss 2.297398\n",
      "iteration 400 / 500: loss 2.296637\n",
      "iteration 0 / 500: loss 84636.462530\n",
      "iteration 100 / 500: loss 2.302127\n",
      "iteration 200 / 500: loss 2.295062\n",
      "iteration 300 / 500: loss 2.294488\n",
      "iteration 400 / 500: loss 2.296648\n",
      "iteration 0 / 500: loss 83838.193179\n",
      "iteration 100 / 500: loss 2.297240\n",
      "iteration 200 / 500: loss 2.298861\n",
      "iteration 300 / 500: loss 2.301400\n",
      "iteration 400 / 500: loss 2.298243\n",
      "iteration 0 / 500: loss 85315.448942\n",
      "iteration 100 / 500: loss 2.301373\n",
      "iteration 200 / 500: loss 2.303248\n",
      "iteration 300 / 500: loss 2.301147\n",
      "iteration 400 / 500: loss 2.297447\n",
      "iteration 0 / 500: loss 82928.924991\n",
      "iteration 100 / 500: loss 2.304120\n",
      "iteration 200 / 500: loss 2.299662\n",
      "iteration 300 / 500: loss 2.301712\n",
      "iteration 400 / 500: loss 2.296285\n",
      "iteration 0 / 500: loss 84325.918248\n",
      "iteration 100 / 500: loss 2.303643\n",
      "iteration 200 / 500: loss 2.299741\n",
      "iteration 300 / 500: loss 2.298295\n",
      "iteration 400 / 500: loss 2.302651\n",
      "iteration 0 / 500: loss 84527.431352\n",
      "iteration 100 / 500: loss 2.304073\n",
      "iteration 200 / 500: loss 2.306164\n",
      "iteration 300 / 500: loss 2.306217\n",
      "iteration 400 / 500: loss 2.295754\n",
      "iteration 0 / 500: loss 84068.633640\n",
      "iteration 100 / 500: loss 2.305461\n",
      "iteration 200 / 500: loss 2.306016\n",
      "iteration 300 / 500: loss 2.309287\n",
      "iteration 400 / 500: loss 2.309919\n",
      "iteration 0 / 500: loss 83416.944205\n",
      "iteration 100 / 500: loss 2.314573\n",
      "iteration 200 / 500: loss 2.324614\n",
      "iteration 300 / 500: loss 2.323281\n",
      "iteration 400 / 500: loss 2.312947\n",
      "iteration 0 / 500: loss 83751.702860\n",
      "iteration 100 / 500: loss 2.337847\n",
      "iteration 200 / 500: loss 2.344269\n",
      "iteration 300 / 500: loss 2.341631\n",
      "iteration 400 / 500: loss 2.345771\n",
      "iteration 0 / 500: loss 83204.481441\n",
      "iteration 100 / 500: loss 80120.684255\n",
      "iteration 200 / 500: loss 84248.419401\n",
      "iteration 300 / 500: loss 95445.144027\n",
      "iteration 400 / 500: loss inf\n",
      "iteration 0 / 500: loss 83384.105286\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83516.019698\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84357.708751\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 85523.048327\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 82947.870343\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 85785.503476\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84291.204351\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 82480.078076\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83272.976717\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83360.828165\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83354.290921\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83338.719952\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84221.265925\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84390.366240\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83616.077855\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83505.327299\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83595.956514\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84398.465428\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83842.572797\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83827.945290\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84893.548053\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 82958.544950\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84173.163075\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84538.010999\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83126.654331\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83203.260492\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 84190.351256\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 83935.861958\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136959.006205\n",
      "iteration 100 / 500: loss 2.301047\n",
      "iteration 200 / 500: loss 2.299875\n",
      "iteration 300 / 500: loss 2.298234\n",
      "iteration 400 / 500: loss 2.301048\n",
      "iteration 0 / 500: loss 136527.567107\n",
      "iteration 100 / 500: loss 2.300711\n",
      "iteration 200 / 500: loss 2.300346\n",
      "iteration 300 / 500: loss 2.299677\n",
      "iteration 400 / 500: loss 2.300281\n",
      "iteration 0 / 500: loss 137058.524752\n",
      "iteration 100 / 500: loss 2.302603\n",
      "iteration 200 / 500: loss 2.302991\n",
      "iteration 300 / 500: loss 2.303591\n",
      "iteration 400 / 500: loss 2.302092\n",
      "iteration 0 / 500: loss 135766.893911\n",
      "iteration 100 / 500: loss 2.304249\n",
      "iteration 200 / 500: loss 2.301718\n",
      "iteration 300 / 500: loss 2.302644\n",
      "iteration 400 / 500: loss 2.299616\n",
      "iteration 0 / 500: loss 135591.282509\n",
      "iteration 100 / 500: loss 2.306656\n",
      "iteration 200 / 500: loss 2.305220\n",
      "iteration 300 / 500: loss 2.306276\n",
      "iteration 400 / 500: loss 2.299561\n",
      "iteration 0 / 500: loss 133354.647791\n",
      "iteration 100 / 500: loss 2.309131\n",
      "iteration 200 / 500: loss 2.315004\n",
      "iteration 300 / 500: loss 2.310321\n",
      "iteration 400 / 500: loss 2.313171\n",
      "iteration 0 / 500: loss 136312.678679\n",
      "iteration 100 / 500: loss 2.328556\n",
      "iteration 200 / 500: loss 2.325332\n",
      "iteration 300 / 500: loss 2.321832\n",
      "iteration 400 / 500: loss 2.349906\n",
      "iteration 0 / 500: loss 136509.367589\n",
      "iteration 100 / 500: loss 17743463.941852\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137327.440298\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136419.265485\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136982.275269\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135368.449758\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135869.185819\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137881.845569\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136586.707598\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136486.317429\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 134314.004981\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137604.300200\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135503.274260\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135206.792701\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135548.992304\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135512.538750\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136686.953322\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137186.412137\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136911.758330\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 134941.223974\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136725.845501\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137700.806973\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135878.326461\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137086.577397\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137469.814528\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137334.805627\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137985.447210\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136622.249392\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135137.144683\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 136248.017957\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 137638.520838\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135703.899883\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 135878.868563\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 133865.155598\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221402.621420\n",
      "iteration 100 / 500: loss 2.308180\n",
      "iteration 200 / 500: loss 2.305559\n",
      "iteration 300 / 500: loss 2.303338\n",
      "iteration 400 / 500: loss 2.302940\n",
      "iteration 0 / 500: loss 221220.869823\n",
      "iteration 100 / 500: loss 2.310946\n",
      "iteration 200 / 500: loss 2.307571\n",
      "iteration 300 / 500: loss 2.311161\n",
      "iteration 400 / 500: loss 2.306528\n",
      "iteration 0 / 500: loss 222207.317770\n",
      "iteration 100 / 500: loss 2.321920\n",
      "iteration 200 / 500: loss 2.332076\n",
      "iteration 300 / 500: loss 2.323553\n",
      "iteration 400 / 500: loss 2.330600\n",
      "iteration 0 / 500: loss 220594.133826\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220930.229444\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220098.738446\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220531.522569\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221519.554887\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 224824.003567\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 219098.029525\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220505.746802\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 219454.511037\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220383.077277\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 224168.734863\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 222762.335945\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 222242.284843\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 218708.735884\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 222717.986116\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220702.996402\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220418.791658\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221335.819317\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221971.827700\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220360.128762\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 218455.069234\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220923.743848\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 219744.366024\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220279.068191\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 223471.161068\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 222656.808038\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 219974.341876\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220600.174362\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 218578.656169\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 220063.048538\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221788.523051\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 219855.793910\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 222423.668542\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221140.028547\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 221778.919283\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 223351.531121\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 223777.312757\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 355507.508315\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357212.761480\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 358565.078371\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 363001.408812\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 356801.078618\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357911.809979\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 361184.150362\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 359126.346364\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357548.211449\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360068.464547\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360535.692653\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357220.983562\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357153.687854\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 359075.858414\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 359408.150789\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 361619.618757\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360035.939457\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 356573.606504\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 361041.908672\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 356652.463182\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 359344.351362\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 356221.029238\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 354105.349087\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 362173.261001\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360278.514716\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 356546.294900\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 359184.806349\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360860.319893\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 363721.912799\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 359105.278721\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360928.225256\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 355168.944205\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 358179.220232\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 358155.633509\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357966.958105\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357473.685189\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 360957.459516\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 354492.194466\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 364675.556137\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 357130.519504\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 571882.027294\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 585994.965643\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 586580.394077\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 584354.889295\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 580583.163906\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 584548.218285\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 587553.604219\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 588456.783832\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 578935.048283\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 581092.266062\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 580016.314327\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 584793.978135\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 588611.479015\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 584627.518698\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 581985.300705\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 580357.034857\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 586057.596378\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 583157.794685\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 579956.326603\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 582163.910153\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 577017.121468\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 582018.820415\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 590039.370257\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 586693.201572\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 589071.681188\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 590668.658524\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 582046.928989\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 577296.751727\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 580322.465266\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 583650.627091\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 591586.013566\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 577463.301032\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 581817.181260\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 584151.343012\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 577846.635701\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 583494.998678\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 582101.789674\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 578895.282055\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 588844.595052\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 584845.563920\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 951591.079590\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 945835.601483\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 943495.384876\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 937267.479872\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 951771.779647\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 941080.322492\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 928625.779912\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 934781.169856\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 950284.063061\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 931029.293555\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 959599.497796\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949043.114221\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 951696.042882\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 932340.452207\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 940079.004110\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 946187.998271\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 943435.393345\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 935887.862926\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 941550.551962\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 954572.948470\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 961217.234013\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 954694.325849\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949743.110904\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 956072.650934\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 950172.959566\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949979.823727\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949637.284252\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949652.524549\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 953060.116205\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949705.312686\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 952859.839537\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 941023.249194\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 942944.875059\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 945452.401926\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 936902.057342\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 942288.804707\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 949978.435420\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 943542.905283\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 945032.172480\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 939768.375555\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1522125.288816\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1533055.519615\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1525548.851511\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1550805.538885\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1543628.307762\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1533483.374578\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1525171.290980\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1537111.641594\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1531110.040319\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1531845.836207\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1541476.373306\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1546815.053212\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1534165.000657\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1519956.478582\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1519518.401643\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1531601.681341\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1537687.407909\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1532028.370183\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1529641.800075\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1536111.995872\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1519122.837334\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1538092.802930\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1542556.191519\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1533979.422070\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1523050.948649\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1559609.179508\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1529258.335357\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1520927.895807\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1522181.988132\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1553010.201165\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1535422.176909\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1534491.651387\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1518203.378993\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1532239.886352\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1512054.917776\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1553286.077700\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1536092.214894\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1525949.546660\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1530560.856279\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "iteration 0 / 500: loss 1536461.422418\n",
      "iteration 100 / 500: loss nan\n",
      "iteration 200 / 500: loss nan\n",
      "iteration 300 / 500: loss nan\n",
      "iteration 400 / 500: loss nan\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.221327 val accuracy: 0.207000\n",
      "lr 1.000000e-07 reg 1.623777e+04 train accuracy: 0.247265 val accuracy: 0.264000\n",
      "lr 1.000000e-07 reg 2.636651e+04 train accuracy: 0.269857 val accuracy: 0.262000\n",
      "lr 1.000000e-07 reg 4.281332e+04 train accuracy: 0.299959 val accuracy: 0.329000\n",
      "lr 1.000000e-07 reg 6.951928e+04 train accuracy: 0.311735 val accuracy: 0.322000\n",
      "lr 1.000000e-07 reg 1.128838e+05 train accuracy: 0.312857 val accuracy: 0.325000\n",
      "lr 1.000000e-07 reg 1.832981e+05 train accuracy: 0.280714 val accuracy: 0.300000\n",
      "lr 1.000000e-07 reg 2.976351e+05 train accuracy: 0.283714 val accuracy: 0.289000\n",
      "lr 1.000000e-07 reg 4.832930e+05 train accuracy: 0.271837 val accuracy: 0.282000\n",
      "lr 1.000000e-07 reg 7.847600e+05 train accuracy: 0.262857 val accuracy: 0.269000\n",
      "lr 1.000000e-07 reg 1.274275e+06 train accuracy: 0.272286 val accuracy: 0.273000\n",
      "lr 1.000000e-07 reg 2.069138e+06 train accuracy: 0.250408 val accuracy: 0.271000\n",
      "lr 1.000000e-07 reg 3.359818e+06 train accuracy: 0.253061 val accuracy: 0.255000\n",
      "lr 1.000000e-07 reg 5.455595e+06 train accuracy: 0.209490 val accuracy: 0.219000\n",
      "lr 1.000000e-07 reg 8.858668e+06 train accuracy: 0.188204 val accuracy: 0.183000\n",
      "lr 1.000000e-07 reg 1.438450e+07 train accuracy: 0.216000 val accuracy: 0.207000\n",
      "lr 1.000000e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.125336e-07 reg 1.000000e+04 train accuracy: 0.235776 val accuracy: 0.243000\n",
      "lr 1.125336e-07 reg 1.623777e+04 train accuracy: 0.244102 val accuracy: 0.260000\n",
      "lr 1.125336e-07 reg 2.636651e+04 train accuracy: 0.284939 val accuracy: 0.293000\n",
      "lr 1.125336e-07 reg 4.281332e+04 train accuracy: 0.307490 val accuracy: 0.324000\n",
      "lr 1.125336e-07 reg 6.951928e+04 train accuracy: 0.316469 val accuracy: 0.334000\n",
      "lr 1.125336e-07 reg 1.128838e+05 train accuracy: 0.298122 val accuracy: 0.314000\n",
      "lr 1.125336e-07 reg 1.832981e+05 train accuracy: 0.291959 val accuracy: 0.306000\n",
      "lr 1.125336e-07 reg 2.976351e+05 train accuracy: 0.290449 val accuracy: 0.290000\n",
      "lr 1.125336e-07 reg 4.832930e+05 train accuracy: 0.263531 val accuracy: 0.281000\n",
      "lr 1.125336e-07 reg 7.847600e+05 train accuracy: 0.248571 val accuracy: 0.253000\n",
      "lr 1.125336e-07 reg 1.274275e+06 train accuracy: 0.254041 val accuracy: 0.266000\n",
      "lr 1.125336e-07 reg 2.069138e+06 train accuracy: 0.260082 val accuracy: 0.269000\n",
      "lr 1.125336e-07 reg 3.359818e+06 train accuracy: 0.239959 val accuracy: 0.255000\n",
      "lr 1.125336e-07 reg 5.455595e+06 train accuracy: 0.248776 val accuracy: 0.253000\n",
      "lr 1.125336e-07 reg 8.858668e+06 train accuracy: 0.196163 val accuracy: 0.213000\n",
      "lr 1.125336e-07 reg 1.438450e+07 train accuracy: 0.192122 val accuracy: 0.198000\n",
      "lr 1.125336e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.125336e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.125336e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.125336e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.266380e-07 reg 1.000000e+04 train accuracy: 0.230918 val accuracy: 0.213000\n",
      "lr 1.266380e-07 reg 1.623777e+04 train accuracy: 0.260918 val accuracy: 0.258000\n",
      "lr 1.266380e-07 reg 2.636651e+04 train accuracy: 0.292041 val accuracy: 0.286000\n",
      "lr 1.266380e-07 reg 4.281332e+04 train accuracy: 0.323531 val accuracy: 0.338000\n",
      "lr 1.266380e-07 reg 6.951928e+04 train accuracy: 0.315388 val accuracy: 0.334000\n",
      "lr 1.266380e-07 reg 1.128838e+05 train accuracy: 0.307796 val accuracy: 0.327000\n",
      "lr 1.266380e-07 reg 1.832981e+05 train accuracy: 0.292367 val accuracy: 0.308000\n",
      "lr 1.266380e-07 reg 2.976351e+05 train accuracy: 0.270265 val accuracy: 0.284000\n",
      "lr 1.266380e-07 reg 4.832930e+05 train accuracy: 0.280939 val accuracy: 0.292000\n",
      "lr 1.266380e-07 reg 7.847600e+05 train accuracy: 0.249633 val accuracy: 0.259000\n",
      "lr 1.266380e-07 reg 1.274275e+06 train accuracy: 0.248122 val accuracy: 0.261000\n",
      "lr 1.266380e-07 reg 2.069138e+06 train accuracy: 0.248163 val accuracy: 0.276000\n",
      "lr 1.266380e-07 reg 3.359818e+06 train accuracy: 0.232796 val accuracy: 0.245000\n",
      "lr 1.266380e-07 reg 5.455595e+06 train accuracy: 0.224755 val accuracy: 0.241000\n",
      "lr 1.266380e-07 reg 8.858668e+06 train accuracy: 0.222837 val accuracy: 0.218000\n",
      "lr 1.266380e-07 reg 1.438450e+07 train accuracy: 0.126184 val accuracy: 0.135000\n",
      "lr 1.266380e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.266380e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.266380e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.266380e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.425103e-07 reg 1.000000e+04 train accuracy: 0.249082 val accuracy: 0.249000\n",
      "lr 1.425103e-07 reg 1.623777e+04 train accuracy: 0.283102 val accuracy: 0.312000\n",
      "lr 1.425103e-07 reg 2.636651e+04 train accuracy: 0.307673 val accuracy: 0.320000\n",
      "lr 1.425103e-07 reg 4.281332e+04 train accuracy: 0.328041 val accuracy: 0.329000\n",
      "lr 1.425103e-07 reg 6.951928e+04 train accuracy: 0.317082 val accuracy: 0.333000\n",
      "lr 1.425103e-07 reg 1.128838e+05 train accuracy: 0.309490 val accuracy: 0.323000\n",
      "lr 1.425103e-07 reg 1.832981e+05 train accuracy: 0.286673 val accuracy: 0.299000\n",
      "lr 1.425103e-07 reg 2.976351e+05 train accuracy: 0.270776 val accuracy: 0.286000\n",
      "lr 1.425103e-07 reg 4.832930e+05 train accuracy: 0.265980 val accuracy: 0.275000\n",
      "lr 1.425103e-07 reg 7.847600e+05 train accuracy: 0.281388 val accuracy: 0.292000\n",
      "lr 1.425103e-07 reg 1.274275e+06 train accuracy: 0.256408 val accuracy: 0.277000\n",
      "lr 1.425103e-07 reg 2.069138e+06 train accuracy: 0.265653 val accuracy: 0.278000\n",
      "lr 1.425103e-07 reg 3.359818e+06 train accuracy: 0.240837 val accuracy: 0.270000\n",
      "lr 1.425103e-07 reg 5.455595e+06 train accuracy: 0.220367 val accuracy: 0.224000\n",
      "lr 1.425103e-07 reg 8.858668e+06 train accuracy: 0.229163 val accuracy: 0.230000\n",
      "lr 1.425103e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.425103e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.425103e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.425103e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.425103e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.603719e-07 reg 1.000000e+04 train accuracy: 0.268224 val accuracy: 0.274000\n",
      "lr 1.603719e-07 reg 1.623777e+04 train accuracy: 0.295327 val accuracy: 0.311000\n",
      "lr 1.603719e-07 reg 2.636651e+04 train accuracy: 0.322000 val accuracy: 0.335000\n",
      "lr 1.603719e-07 reg 4.281332e+04 train accuracy: 0.333551 val accuracy: 0.346000\n",
      "lr 1.603719e-07 reg 6.951928e+04 train accuracy: 0.319878 val accuracy: 0.333000\n",
      "lr 1.603719e-07 reg 1.128838e+05 train accuracy: 0.307469 val accuracy: 0.320000\n",
      "lr 1.603719e-07 reg 1.832981e+05 train accuracy: 0.292449 val accuracy: 0.309000\n",
      "lr 1.603719e-07 reg 2.976351e+05 train accuracy: 0.281224 val accuracy: 0.286000\n",
      "lr 1.603719e-07 reg 4.832930e+05 train accuracy: 0.257918 val accuracy: 0.271000\n",
      "lr 1.603719e-07 reg 7.847600e+05 train accuracy: 0.277143 val accuracy: 0.289000\n",
      "lr 1.603719e-07 reg 1.274275e+06 train accuracy: 0.260592 val accuracy: 0.272000\n",
      "lr 1.603719e-07 reg 2.069138e+06 train accuracy: 0.245612 val accuracy: 0.262000\n",
      "lr 1.603719e-07 reg 3.359818e+06 train accuracy: 0.243653 val accuracy: 0.244000\n",
      "lr 1.603719e-07 reg 5.455595e+06 train accuracy: 0.245184 val accuracy: 0.261000\n",
      "lr 1.603719e-07 reg 8.858668e+06 train accuracy: 0.190510 val accuracy: 0.184000\n",
      "lr 1.603719e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.603719e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.603719e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.603719e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.603719e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.804722e-07 reg 1.000000e+04 train accuracy: 0.270020 val accuracy: 0.292000\n",
      "lr 1.804722e-07 reg 1.623777e+04 train accuracy: 0.304939 val accuracy: 0.308000\n",
      "lr 1.804722e-07 reg 2.636651e+04 train accuracy: 0.331490 val accuracy: 0.327000\n",
      "lr 1.804722e-07 reg 4.281332e+04 train accuracy: 0.328878 val accuracy: 0.339000\n",
      "lr 1.804722e-07 reg 6.951928e+04 train accuracy: 0.316959 val accuracy: 0.318000\n",
      "lr 1.804722e-07 reg 1.128838e+05 train accuracy: 0.304204 val accuracy: 0.322000\n",
      "lr 1.804722e-07 reg 1.832981e+05 train accuracy: 0.288510 val accuracy: 0.294000\n",
      "lr 1.804722e-07 reg 2.976351e+05 train accuracy: 0.270816 val accuracy: 0.277000\n",
      "lr 1.804722e-07 reg 4.832930e+05 train accuracy: 0.274122 val accuracy: 0.281000\n",
      "lr 1.804722e-07 reg 7.847600e+05 train accuracy: 0.260265 val accuracy: 0.268000\n",
      "lr 1.804722e-07 reg 1.274275e+06 train accuracy: 0.241327 val accuracy: 0.249000\n",
      "lr 1.804722e-07 reg 2.069138e+06 train accuracy: 0.252735 val accuracy: 0.260000\n",
      "lr 1.804722e-07 reg 3.359818e+06 train accuracy: 0.242265 val accuracy: 0.247000\n",
      "lr 1.804722e-07 reg 5.455595e+06 train accuracy: 0.196816 val accuracy: 0.207000\n",
      "lr 1.804722e-07 reg 8.858668e+06 train accuracy: 0.183571 val accuracy: 0.184000\n",
      "lr 1.804722e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.804722e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.804722e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.804722e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.804722e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.030918e-07 reg 1.000000e+04 train accuracy: 0.291449 val accuracy: 0.313000\n",
      "lr 2.030918e-07 reg 1.623777e+04 train accuracy: 0.321857 val accuracy: 0.352000\n",
      "lr 2.030918e-07 reg 2.636651e+04 train accuracy: 0.340388 val accuracy: 0.354000\n",
      "lr 2.030918e-07 reg 4.281332e+04 train accuracy: 0.332898 val accuracy: 0.354000\n",
      "lr 2.030918e-07 reg 6.951928e+04 train accuracy: 0.315490 val accuracy: 0.333000\n",
      "lr 2.030918e-07 reg 1.128838e+05 train accuracy: 0.299469 val accuracy: 0.313000\n",
      "lr 2.030918e-07 reg 1.832981e+05 train accuracy: 0.289184 val accuracy: 0.304000\n",
      "lr 2.030918e-07 reg 2.976351e+05 train accuracy: 0.275592 val accuracy: 0.288000\n",
      "lr 2.030918e-07 reg 4.832930e+05 train accuracy: 0.272653 val accuracy: 0.286000\n",
      "lr 2.030918e-07 reg 7.847600e+05 train accuracy: 0.259163 val accuracy: 0.272000\n",
      "lr 2.030918e-07 reg 1.274275e+06 train accuracy: 0.275510 val accuracy: 0.291000\n",
      "lr 2.030918e-07 reg 2.069138e+06 train accuracy: 0.234245 val accuracy: 0.249000\n",
      "lr 2.030918e-07 reg 3.359818e+06 train accuracy: 0.217673 val accuracy: 0.243000\n",
      "lr 2.030918e-07 reg 5.455595e+06 train accuracy: 0.239735 val accuracy: 0.248000\n",
      "lr 2.030918e-07 reg 8.858668e+06 train accuracy: 0.174673 val accuracy: 0.166000\n",
      "lr 2.030918e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.030918e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.030918e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.030918e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.030918e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.285464e-07 reg 1.000000e+04 train accuracy: 0.298939 val accuracy: 0.295000\n",
      "lr 2.285464e-07 reg 1.623777e+04 train accuracy: 0.328469 val accuracy: 0.359000\n",
      "lr 2.285464e-07 reg 2.636651e+04 train accuracy: 0.339388 val accuracy: 0.343000\n",
      "lr 2.285464e-07 reg 4.281332e+04 train accuracy: 0.337510 val accuracy: 0.352000\n",
      "lr 2.285464e-07 reg 6.951928e+04 train accuracy: 0.313551 val accuracy: 0.333000\n",
      "lr 2.285464e-07 reg 1.128838e+05 train accuracy: 0.305939 val accuracy: 0.330000\n",
      "lr 2.285464e-07 reg 1.832981e+05 train accuracy: 0.293857 val accuracy: 0.315000\n",
      "lr 2.285464e-07 reg 2.976351e+05 train accuracy: 0.280306 val accuracy: 0.289000\n",
      "lr 2.285464e-07 reg 4.832930e+05 train accuracy: 0.263612 val accuracy: 0.286000\n",
      "lr 2.285464e-07 reg 7.847600e+05 train accuracy: 0.261082 val accuracy: 0.284000\n",
      "lr 2.285464e-07 reg 1.274275e+06 train accuracy: 0.232735 val accuracy: 0.228000\n",
      "lr 2.285464e-07 reg 2.069138e+06 train accuracy: 0.225755 val accuracy: 0.221000\n",
      "lr 2.285464e-07 reg 3.359818e+06 train accuracy: 0.226653 val accuracy: 0.236000\n",
      "lr 2.285464e-07 reg 5.455595e+06 train accuracy: 0.188388 val accuracy: 0.171000\n",
      "lr 2.285464e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.285464e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.285464e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.285464e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.285464e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.285464e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.571914e-07 reg 1.000000e+04 train accuracy: 0.314327 val accuracy: 0.315000\n",
      "lr 2.571914e-07 reg 1.623777e+04 train accuracy: 0.344939 val accuracy: 0.349000\n",
      "lr 2.571914e-07 reg 2.636651e+04 train accuracy: 0.349776 val accuracy: 0.358000\n",
      "lr 2.571914e-07 reg 4.281332e+04 train accuracy: 0.337000 val accuracy: 0.357000\n",
      "lr 2.571914e-07 reg 6.951928e+04 train accuracy: 0.322000 val accuracy: 0.336000\n",
      "lr 2.571914e-07 reg 1.128838e+05 train accuracy: 0.311449 val accuracy: 0.322000\n",
      "lr 2.571914e-07 reg 1.832981e+05 train accuracy: 0.305327 val accuracy: 0.316000\n",
      "lr 2.571914e-07 reg 2.976351e+05 train accuracy: 0.273510 val accuracy: 0.288000\n",
      "lr 2.571914e-07 reg 4.832930e+05 train accuracy: 0.267429 val accuracy: 0.281000\n",
      "lr 2.571914e-07 reg 7.847600e+05 train accuracy: 0.266306 val accuracy: 0.274000\n",
      "lr 2.571914e-07 reg 1.274275e+06 train accuracy: 0.253469 val accuracy: 0.261000\n",
      "lr 2.571914e-07 reg 2.069138e+06 train accuracy: 0.238776 val accuracy: 0.263000\n",
      "lr 2.571914e-07 reg 3.359818e+06 train accuracy: 0.233673 val accuracy: 0.246000\n",
      "lr 2.571914e-07 reg 5.455595e+06 train accuracy: 0.186551 val accuracy: 0.195000\n",
      "lr 2.571914e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.571914e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.571914e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.571914e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.571914e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.571914e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.894266e-07 reg 1.000000e+04 train accuracy: 0.331000 val accuracy: 0.347000\n",
      "lr 2.894266e-07 reg 1.623777e+04 train accuracy: 0.351041 val accuracy: 0.358000\n",
      "lr 2.894266e-07 reg 2.636651e+04 train accuracy: 0.347041 val accuracy: 0.364000\n",
      "lr 2.894266e-07 reg 4.281332e+04 train accuracy: 0.335776 val accuracy: 0.342000\n",
      "lr 2.894266e-07 reg 6.951928e+04 train accuracy: 0.312735 val accuracy: 0.329000\n",
      "lr 2.894266e-07 reg 1.128838e+05 train accuracy: 0.304878 val accuracy: 0.319000\n",
      "lr 2.894266e-07 reg 1.832981e+05 train accuracy: 0.291531 val accuracy: 0.301000\n",
      "lr 2.894266e-07 reg 2.976351e+05 train accuracy: 0.270000 val accuracy: 0.287000\n",
      "lr 2.894266e-07 reg 4.832930e+05 train accuracy: 0.266449 val accuracy: 0.278000\n",
      "lr 2.894266e-07 reg 7.847600e+05 train accuracy: 0.249776 val accuracy: 0.274000\n",
      "lr 2.894266e-07 reg 1.274275e+06 train accuracy: 0.248918 val accuracy: 0.264000\n",
      "lr 2.894266e-07 reg 2.069138e+06 train accuracy: 0.239878 val accuracy: 0.235000\n",
      "lr 2.894266e-07 reg 3.359818e+06 train accuracy: 0.200490 val accuracy: 0.194000\n",
      "lr 2.894266e-07 reg 5.455595e+06 train accuracy: 0.185184 val accuracy: 0.168000\n",
      "lr 2.894266e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.894266e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.894266e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.894266e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.894266e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.894266e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.257021e-07 reg 1.000000e+04 train accuracy: 0.336306 val accuracy: 0.344000\n",
      "lr 3.257021e-07 reg 1.623777e+04 train accuracy: 0.356449 val accuracy: 0.369000\n",
      "lr 3.257021e-07 reg 2.636651e+04 train accuracy: 0.349286 val accuracy: 0.364000\n",
      "lr 3.257021e-07 reg 4.281332e+04 train accuracy: 0.331204 val accuracy: 0.338000\n",
      "lr 3.257021e-07 reg 6.951928e+04 train accuracy: 0.324980 val accuracy: 0.330000\n",
      "lr 3.257021e-07 reg 1.128838e+05 train accuracy: 0.294592 val accuracy: 0.305000\n",
      "lr 3.257021e-07 reg 1.832981e+05 train accuracy: 0.288143 val accuracy: 0.301000\n",
      "lr 3.257021e-07 reg 2.976351e+05 train accuracy: 0.277429 val accuracy: 0.295000\n",
      "lr 3.257021e-07 reg 4.832930e+05 train accuracy: 0.262857 val accuracy: 0.258000\n",
      "lr 3.257021e-07 reg 7.847600e+05 train accuracy: 0.273204 val accuracy: 0.285000\n",
      "lr 3.257021e-07 reg 1.274275e+06 train accuracy: 0.246837 val accuracy: 0.259000\n",
      "lr 3.257021e-07 reg 2.069138e+06 train accuracy: 0.240286 val accuracy: 0.240000\n",
      "lr 3.257021e-07 reg 3.359818e+06 train accuracy: 0.217510 val accuracy: 0.216000\n",
      "lr 3.257021e-07 reg 5.455595e+06 train accuracy: 0.155776 val accuracy: 0.146000\n",
      "lr 3.257021e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.257021e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.257021e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.257021e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.257021e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.257021e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.665241e-07 reg 1.000000e+04 train accuracy: 0.349265 val accuracy: 0.348000\n",
      "lr 3.665241e-07 reg 1.623777e+04 train accuracy: 0.360755 val accuracy: 0.377000\n",
      "lr 3.665241e-07 reg 2.636651e+04 train accuracy: 0.353939 val accuracy: 0.359000\n",
      "lr 3.665241e-07 reg 4.281332e+04 train accuracy: 0.328898 val accuracy: 0.341000\n",
      "lr 3.665241e-07 reg 6.951928e+04 train accuracy: 0.312857 val accuracy: 0.325000\n",
      "lr 3.665241e-07 reg 1.128838e+05 train accuracy: 0.305469 val accuracy: 0.318000\n",
      "lr 3.665241e-07 reg 1.832981e+05 train accuracy: 0.283837 val accuracy: 0.297000\n",
      "lr 3.665241e-07 reg 2.976351e+05 train accuracy: 0.276408 val accuracy: 0.292000\n",
      "lr 3.665241e-07 reg 4.832930e+05 train accuracy: 0.256082 val accuracy: 0.269000\n",
      "lr 3.665241e-07 reg 7.847600e+05 train accuracy: 0.257714 val accuracy: 0.276000\n",
      "lr 3.665241e-07 reg 1.274275e+06 train accuracy: 0.256918 val accuracy: 0.268000\n",
      "lr 3.665241e-07 reg 2.069138e+06 train accuracy: 0.217694 val accuracy: 0.229000\n",
      "lr 3.665241e-07 reg 3.359818e+06 train accuracy: 0.225204 val accuracy: 0.226000\n",
      "lr 3.665241e-07 reg 5.455595e+06 train accuracy: 0.095224 val accuracy: 0.094000\n",
      "lr 3.665241e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.665241e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.665241e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.665241e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.665241e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.665241e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 1.000000e+04 train accuracy: 0.362122 val accuracy: 0.363000\n",
      "lr 4.124626e-07 reg 1.623777e+04 train accuracy: 0.354878 val accuracy: 0.361000\n",
      "lr 4.124626e-07 reg 2.636651e+04 train accuracy: 0.351204 val accuracy: 0.365000\n",
      "lr 4.124626e-07 reg 4.281332e+04 train accuracy: 0.320714 val accuracy: 0.324000\n",
      "lr 4.124626e-07 reg 6.951928e+04 train accuracy: 0.309878 val accuracy: 0.326000\n",
      "lr 4.124626e-07 reg 1.128838e+05 train accuracy: 0.297755 val accuracy: 0.307000\n",
      "lr 4.124626e-07 reg 1.832981e+05 train accuracy: 0.291898 val accuracy: 0.297000\n",
      "lr 4.124626e-07 reg 2.976351e+05 train accuracy: 0.283878 val accuracy: 0.302000\n",
      "lr 4.124626e-07 reg 4.832930e+05 train accuracy: 0.265816 val accuracy: 0.289000\n",
      "lr 4.124626e-07 reg 7.847600e+05 train accuracy: 0.258327 val accuracy: 0.267000\n",
      "lr 4.124626e-07 reg 1.274275e+06 train accuracy: 0.257265 val accuracy: 0.268000\n",
      "lr 4.124626e-07 reg 2.069138e+06 train accuracy: 0.211429 val accuracy: 0.217000\n",
      "lr 4.124626e-07 reg 3.359818e+06 train accuracy: 0.196388 val accuracy: 0.187000\n",
      "lr 4.124626e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.124626e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 1.000000e+04 train accuracy: 0.363878 val accuracy: 0.384000\n",
      "lr 4.641589e-07 reg 1.623777e+04 train accuracy: 0.357327 val accuracy: 0.368000\n",
      "lr 4.641589e-07 reg 2.636651e+04 train accuracy: 0.350082 val accuracy: 0.362000\n",
      "lr 4.641589e-07 reg 4.281332e+04 train accuracy: 0.332286 val accuracy: 0.343000\n",
      "lr 4.641589e-07 reg 6.951928e+04 train accuracy: 0.305265 val accuracy: 0.312000\n",
      "lr 4.641589e-07 reg 1.128838e+05 train accuracy: 0.305306 val accuracy: 0.313000\n",
      "lr 4.641589e-07 reg 1.832981e+05 train accuracy: 0.283755 val accuracy: 0.303000\n",
      "lr 4.641589e-07 reg 2.976351e+05 train accuracy: 0.288020 val accuracy: 0.291000\n",
      "lr 4.641589e-07 reg 4.832930e+05 train accuracy: 0.264306 val accuracy: 0.272000\n",
      "lr 4.641589e-07 reg 7.847600e+05 train accuracy: 0.254837 val accuracy: 0.260000\n",
      "lr 4.641589e-07 reg 1.274275e+06 train accuracy: 0.236082 val accuracy: 0.255000\n",
      "lr 4.641589e-07 reg 2.069138e+06 train accuracy: 0.226408 val accuracy: 0.231000\n",
      "lr 4.641589e-07 reg 3.359818e+06 train accuracy: 0.174327 val accuracy: 0.150000\n",
      "lr 4.641589e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 1.000000e+04 train accuracy: 0.364163 val accuracy: 0.369000\n",
      "lr 5.223345e-07 reg 1.623777e+04 train accuracy: 0.355163 val accuracy: 0.371000\n",
      "lr 5.223345e-07 reg 2.636651e+04 train accuracy: 0.345612 val accuracy: 0.356000\n",
      "lr 5.223345e-07 reg 4.281332e+04 train accuracy: 0.331286 val accuracy: 0.331000\n",
      "lr 5.223345e-07 reg 6.951928e+04 train accuracy: 0.308959 val accuracy: 0.324000\n",
      "lr 5.223345e-07 reg 1.128838e+05 train accuracy: 0.299653 val accuracy: 0.318000\n",
      "lr 5.223345e-07 reg 1.832981e+05 train accuracy: 0.292286 val accuracy: 0.287000\n",
      "lr 5.223345e-07 reg 2.976351e+05 train accuracy: 0.269735 val accuracy: 0.292000\n",
      "lr 5.223345e-07 reg 4.832930e+05 train accuracy: 0.254592 val accuracy: 0.272000\n",
      "lr 5.223345e-07 reg 7.847600e+05 train accuracy: 0.242653 val accuracy: 0.251000\n",
      "lr 5.223345e-07 reg 1.274275e+06 train accuracy: 0.232714 val accuracy: 0.251000\n",
      "lr 5.223345e-07 reg 2.069138e+06 train accuracy: 0.205510 val accuracy: 0.202000\n",
      "lr 5.223345e-07 reg 3.359818e+06 train accuracy: 0.131388 val accuracy: 0.137000\n",
      "lr 5.223345e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.223345e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 1.000000e+04 train accuracy: 0.370122 val accuracy: 0.378000\n",
      "lr 5.878016e-07 reg 1.623777e+04 train accuracy: 0.352551 val accuracy: 0.374000\n",
      "lr 5.878016e-07 reg 2.636651e+04 train accuracy: 0.356531 val accuracy: 0.364000\n",
      "lr 5.878016e-07 reg 4.281332e+04 train accuracy: 0.332102 val accuracy: 0.339000\n",
      "lr 5.878016e-07 reg 6.951928e+04 train accuracy: 0.293612 val accuracy: 0.306000\n",
      "lr 5.878016e-07 reg 1.128838e+05 train accuracy: 0.297163 val accuracy: 0.315000\n",
      "lr 5.878016e-07 reg 1.832981e+05 train accuracy: 0.285306 val accuracy: 0.291000\n",
      "lr 5.878016e-07 reg 2.976351e+05 train accuracy: 0.254980 val accuracy: 0.257000\n",
      "lr 5.878016e-07 reg 4.832930e+05 train accuracy: 0.259327 val accuracy: 0.265000\n",
      "lr 5.878016e-07 reg 7.847600e+05 train accuracy: 0.242898 val accuracy: 0.248000\n",
      "lr 5.878016e-07 reg 1.274275e+06 train accuracy: 0.220020 val accuracy: 0.226000\n",
      "lr 5.878016e-07 reg 2.069138e+06 train accuracy: 0.200878 val accuracy: 0.199000\n",
      "lr 5.878016e-07 reg 3.359818e+06 train accuracy: 0.120122 val accuracy: 0.132000\n",
      "lr 5.878016e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.878016e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 1.000000e+04 train accuracy: 0.368796 val accuracy: 0.387000\n",
      "lr 6.614741e-07 reg 1.623777e+04 train accuracy: 0.356490 val accuracy: 0.380000\n",
      "lr 6.614741e-07 reg 2.636651e+04 train accuracy: 0.338510 val accuracy: 0.346000\n",
      "lr 6.614741e-07 reg 4.281332e+04 train accuracy: 0.333449 val accuracy: 0.342000\n",
      "lr 6.614741e-07 reg 6.951928e+04 train accuracy: 0.315408 val accuracy: 0.322000\n",
      "lr 6.614741e-07 reg 1.128838e+05 train accuracy: 0.283490 val accuracy: 0.295000\n",
      "lr 6.614741e-07 reg 1.832981e+05 train accuracy: 0.280898 val accuracy: 0.286000\n",
      "lr 6.614741e-07 reg 2.976351e+05 train accuracy: 0.242735 val accuracy: 0.264000\n",
      "lr 6.614741e-07 reg 4.832930e+05 train accuracy: 0.258878 val accuracy: 0.285000\n",
      "lr 6.614741e-07 reg 7.847600e+05 train accuracy: 0.224000 val accuracy: 0.231000\n",
      "lr 6.614741e-07 reg 1.274275e+06 train accuracy: 0.208878 val accuracy: 0.209000\n",
      "lr 6.614741e-07 reg 2.069138e+06 train accuracy: 0.202959 val accuracy: 0.193000\n",
      "lr 6.614741e-07 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.614741e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 1.000000e+04 train accuracy: 0.368020 val accuracy: 0.371000\n",
      "lr 7.443803e-07 reg 1.623777e+04 train accuracy: 0.350143 val accuracy: 0.353000\n",
      "lr 7.443803e-07 reg 2.636651e+04 train accuracy: 0.338327 val accuracy: 0.346000\n",
      "lr 7.443803e-07 reg 4.281332e+04 train accuracy: 0.329959 val accuracy: 0.332000\n",
      "lr 7.443803e-07 reg 6.951928e+04 train accuracy: 0.305673 val accuracy: 0.315000\n",
      "lr 7.443803e-07 reg 1.128838e+05 train accuracy: 0.307837 val accuracy: 0.324000\n",
      "lr 7.443803e-07 reg 1.832981e+05 train accuracy: 0.288082 val accuracy: 0.293000\n",
      "lr 7.443803e-07 reg 2.976351e+05 train accuracy: 0.267204 val accuracy: 0.266000\n",
      "lr 7.443803e-07 reg 4.832930e+05 train accuracy: 0.231367 val accuracy: 0.228000\n",
      "lr 7.443803e-07 reg 7.847600e+05 train accuracy: 0.260122 val accuracy: 0.266000\n",
      "lr 7.443803e-07 reg 1.274275e+06 train accuracy: 0.228633 val accuracy: 0.250000\n",
      "lr 7.443803e-07 reg 2.069138e+06 train accuracy: 0.179857 val accuracy: 0.213000\n",
      "lr 7.443803e-07 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.443803e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 1.000000e+04 train accuracy: 0.362816 val accuracy: 0.356000\n",
      "lr 8.376776e-07 reg 1.623777e+04 train accuracy: 0.350163 val accuracy: 0.365000\n",
      "lr 8.376776e-07 reg 2.636651e+04 train accuracy: 0.340347 val accuracy: 0.357000\n",
      "lr 8.376776e-07 reg 4.281332e+04 train accuracy: 0.315367 val accuracy: 0.330000\n",
      "lr 8.376776e-07 reg 6.951928e+04 train accuracy: 0.305388 val accuracy: 0.316000\n",
      "lr 8.376776e-07 reg 1.128838e+05 train accuracy: 0.291633 val accuracy: 0.301000\n",
      "lr 8.376776e-07 reg 1.832981e+05 train accuracy: 0.265429 val accuracy: 0.268000\n",
      "lr 8.376776e-07 reg 2.976351e+05 train accuracy: 0.278163 val accuracy: 0.278000\n",
      "lr 8.376776e-07 reg 4.832930e+05 train accuracy: 0.267714 val accuracy: 0.285000\n",
      "lr 8.376776e-07 reg 7.847600e+05 train accuracy: 0.248571 val accuracy: 0.248000\n",
      "lr 8.376776e-07 reg 1.274275e+06 train accuracy: 0.210041 val accuracy: 0.221000\n",
      "lr 8.376776e-07 reg 2.069138e+06 train accuracy: 0.100490 val accuracy: 0.097000\n",
      "lr 8.376776e-07 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.376776e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 1.000000e+04 train accuracy: 0.372735 val accuracy: 0.379000\n",
      "lr 9.426685e-07 reg 1.623777e+04 train accuracy: 0.353673 val accuracy: 0.367000\n",
      "lr 9.426685e-07 reg 2.636651e+04 train accuracy: 0.338776 val accuracy: 0.363000\n",
      "lr 9.426685e-07 reg 4.281332e+04 train accuracy: 0.328714 val accuracy: 0.338000\n",
      "lr 9.426685e-07 reg 6.951928e+04 train accuracy: 0.317061 val accuracy: 0.333000\n",
      "lr 9.426685e-07 reg 1.128838e+05 train accuracy: 0.293286 val accuracy: 0.316000\n",
      "lr 9.426685e-07 reg 1.832981e+05 train accuracy: 0.284082 val accuracy: 0.295000\n",
      "lr 9.426685e-07 reg 2.976351e+05 train accuracy: 0.248388 val accuracy: 0.264000\n",
      "lr 9.426685e-07 reg 4.832930e+05 train accuracy: 0.246980 val accuracy: 0.248000\n",
      "lr 9.426685e-07 reg 7.847600e+05 train accuracy: 0.219490 val accuracy: 0.218000\n",
      "lr 9.426685e-07 reg 1.274275e+06 train accuracy: 0.179041 val accuracy: 0.183000\n",
      "lr 9.426685e-07 reg 2.069138e+06 train accuracy: 0.083082 val accuracy: 0.075000\n",
      "lr 9.426685e-07 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 9.426685e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 1.000000e+04 train accuracy: 0.372837 val accuracy: 0.376000\n",
      "lr 1.060818e-06 reg 1.623777e+04 train accuracy: 0.350959 val accuracy: 0.354000\n",
      "lr 1.060818e-06 reg 2.636651e+04 train accuracy: 0.340755 val accuracy: 0.339000\n",
      "lr 1.060818e-06 reg 4.281332e+04 train accuracy: 0.314000 val accuracy: 0.332000\n",
      "lr 1.060818e-06 reg 6.951928e+04 train accuracy: 0.300224 val accuracy: 0.311000\n",
      "lr 1.060818e-06 reg 1.128838e+05 train accuracy: 0.295612 val accuracy: 0.312000\n",
      "lr 1.060818e-06 reg 1.832981e+05 train accuracy: 0.276449 val accuracy: 0.289000\n",
      "lr 1.060818e-06 reg 2.976351e+05 train accuracy: 0.264796 val accuracy: 0.270000\n",
      "lr 1.060818e-06 reg 4.832930e+05 train accuracy: 0.243020 val accuracy: 0.253000\n",
      "lr 1.060818e-06 reg 7.847600e+05 train accuracy: 0.225959 val accuracy: 0.232000\n",
      "lr 1.060818e-06 reg 1.274275e+06 train accuracy: 0.133776 val accuracy: 0.138000\n",
      "lr 1.060818e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.060818e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 1.000000e+04 train accuracy: 0.360041 val accuracy: 0.357000\n",
      "lr 1.193777e-06 reg 1.623777e+04 train accuracy: 0.351878 val accuracy: 0.364000\n",
      "lr 1.193777e-06 reg 2.636651e+04 train accuracy: 0.334673 val accuracy: 0.356000\n",
      "lr 1.193777e-06 reg 4.281332e+04 train accuracy: 0.321735 val accuracy: 0.336000\n",
      "lr 1.193777e-06 reg 6.951928e+04 train accuracy: 0.317531 val accuracy: 0.330000\n",
      "lr 1.193777e-06 reg 1.128838e+05 train accuracy: 0.289143 val accuracy: 0.289000\n",
      "lr 1.193777e-06 reg 1.832981e+05 train accuracy: 0.280061 val accuracy: 0.299000\n",
      "lr 1.193777e-06 reg 2.976351e+05 train accuracy: 0.262531 val accuracy: 0.276000\n",
      "lr 1.193777e-06 reg 4.832930e+05 train accuracy: 0.264286 val accuracy: 0.273000\n",
      "lr 1.193777e-06 reg 7.847600e+05 train accuracy: 0.213592 val accuracy: 0.223000\n",
      "lr 1.193777e-06 reg 1.274275e+06 train accuracy: 0.099755 val accuracy: 0.079000\n",
      "lr 1.193777e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.193777e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 1.000000e+04 train accuracy: 0.358531 val accuracy: 0.373000\n",
      "lr 1.343399e-06 reg 1.623777e+04 train accuracy: 0.348633 val accuracy: 0.367000\n",
      "lr 1.343399e-06 reg 2.636651e+04 train accuracy: 0.329184 val accuracy: 0.346000\n",
      "lr 1.343399e-06 reg 4.281332e+04 train accuracy: 0.328612 val accuracy: 0.335000\n",
      "lr 1.343399e-06 reg 6.951928e+04 train accuracy: 0.292204 val accuracy: 0.310000\n",
      "lr 1.343399e-06 reg 1.128838e+05 train accuracy: 0.278204 val accuracy: 0.303000\n",
      "lr 1.343399e-06 reg 1.832981e+05 train accuracy: 0.273041 val accuracy: 0.279000\n",
      "lr 1.343399e-06 reg 2.976351e+05 train accuracy: 0.251612 val accuracy: 0.255000\n",
      "lr 1.343399e-06 reg 4.832930e+05 train accuracy: 0.236571 val accuracy: 0.250000\n",
      "lr 1.343399e-06 reg 7.847600e+05 train accuracy: 0.205735 val accuracy: 0.205000\n",
      "lr 1.343399e-06 reg 1.274275e+06 train accuracy: 0.052653 val accuracy: 0.046000\n",
      "lr 1.343399e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.343399e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 1.000000e+04 train accuracy: 0.359388 val accuracy: 0.371000\n",
      "lr 1.511775e-06 reg 1.623777e+04 train accuracy: 0.344592 val accuracy: 0.345000\n",
      "lr 1.511775e-06 reg 2.636651e+04 train accuracy: 0.335796 val accuracy: 0.346000\n",
      "lr 1.511775e-06 reg 4.281332e+04 train accuracy: 0.314673 val accuracy: 0.321000\n",
      "lr 1.511775e-06 reg 6.951928e+04 train accuracy: 0.313592 val accuracy: 0.316000\n",
      "lr 1.511775e-06 reg 1.128838e+05 train accuracy: 0.289857 val accuracy: 0.286000\n",
      "lr 1.511775e-06 reg 1.832981e+05 train accuracy: 0.242939 val accuracy: 0.260000\n",
      "lr 1.511775e-06 reg 2.976351e+05 train accuracy: 0.241898 val accuracy: 0.241000\n",
      "lr 1.511775e-06 reg 4.832930e+05 train accuracy: 0.223306 val accuracy: 0.233000\n",
      "lr 1.511775e-06 reg 7.847600e+05 train accuracy: 0.203878 val accuracy: 0.201000\n",
      "lr 1.511775e-06 reg 1.274275e+06 train accuracy: 0.078347 val accuracy: 0.067000\n",
      "lr 1.511775e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.511775e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 1.000000e+04 train accuracy: 0.359469 val accuracy: 0.359000\n",
      "lr 1.701254e-06 reg 1.623777e+04 train accuracy: 0.343388 val accuracy: 0.345000\n",
      "lr 1.701254e-06 reg 2.636651e+04 train accuracy: 0.329061 val accuracy: 0.350000\n",
      "lr 1.701254e-06 reg 4.281332e+04 train accuracy: 0.312000 val accuracy: 0.329000\n",
      "lr 1.701254e-06 reg 6.951928e+04 train accuracy: 0.294796 val accuracy: 0.298000\n",
      "lr 1.701254e-06 reg 1.128838e+05 train accuracy: 0.266776 val accuracy: 0.274000\n",
      "lr 1.701254e-06 reg 1.832981e+05 train accuracy: 0.256245 val accuracy: 0.260000\n",
      "lr 1.701254e-06 reg 2.976351e+05 train accuracy: 0.249347 val accuracy: 0.262000\n",
      "lr 1.701254e-06 reg 4.832930e+05 train accuracy: 0.226531 val accuracy: 0.211000\n",
      "lr 1.701254e-06 reg 7.847600e+05 train accuracy: 0.119388 val accuracy: 0.116000\n",
      "lr 1.701254e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.701254e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 1.000000e+04 train accuracy: 0.365796 val accuracy: 0.377000\n",
      "lr 1.914482e-06 reg 1.623777e+04 train accuracy: 0.326000 val accuracy: 0.331000\n",
      "lr 1.914482e-06 reg 2.636651e+04 train accuracy: 0.313102 val accuracy: 0.327000\n",
      "lr 1.914482e-06 reg 4.281332e+04 train accuracy: 0.302265 val accuracy: 0.320000\n",
      "lr 1.914482e-06 reg 6.951928e+04 train accuracy: 0.301143 val accuracy: 0.302000\n",
      "lr 1.914482e-06 reg 1.128838e+05 train accuracy: 0.266714 val accuracy: 0.278000\n",
      "lr 1.914482e-06 reg 1.832981e+05 train accuracy: 0.266918 val accuracy: 0.265000\n",
      "lr 1.914482e-06 reg 2.976351e+05 train accuracy: 0.219673 val accuracy: 0.243000\n",
      "lr 1.914482e-06 reg 4.832930e+05 train accuracy: 0.196673 val accuracy: 0.204000\n",
      "lr 1.914482e-06 reg 7.847600e+05 train accuracy: 0.068306 val accuracy: 0.066000\n",
      "lr 1.914482e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.914482e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 1.000000e+04 train accuracy: 0.356306 val accuracy: 0.357000\n",
      "lr 2.154435e-06 reg 1.623777e+04 train accuracy: 0.333673 val accuracy: 0.327000\n",
      "lr 2.154435e-06 reg 2.636651e+04 train accuracy: 0.324449 val accuracy: 0.339000\n",
      "lr 2.154435e-06 reg 4.281332e+04 train accuracy: 0.314653 val accuracy: 0.328000\n",
      "lr 2.154435e-06 reg 6.951928e+04 train accuracy: 0.263592 val accuracy: 0.276000\n",
      "lr 2.154435e-06 reg 1.128838e+05 train accuracy: 0.259796 val accuracy: 0.285000\n",
      "lr 2.154435e-06 reg 1.832981e+05 train accuracy: 0.245469 val accuracy: 0.235000\n",
      "lr 2.154435e-06 reg 2.976351e+05 train accuracy: 0.210510 val accuracy: 0.203000\n",
      "lr 2.154435e-06 reg 4.832930e+05 train accuracy: 0.201469 val accuracy: 0.207000\n",
      "lr 2.154435e-06 reg 7.847600e+05 train accuracy: 0.072592 val accuracy: 0.089000\n",
      "lr 2.154435e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 1.000000e+04 train accuracy: 0.352122 val accuracy: 0.374000\n",
      "lr 2.424462e-06 reg 1.623777e+04 train accuracy: 0.337633 val accuracy: 0.345000\n",
      "lr 2.424462e-06 reg 2.636651e+04 train accuracy: 0.295449 val accuracy: 0.293000\n",
      "lr 2.424462e-06 reg 4.281332e+04 train accuracy: 0.305143 val accuracy: 0.313000\n",
      "lr 2.424462e-06 reg 6.951928e+04 train accuracy: 0.280837 val accuracy: 0.287000\n",
      "lr 2.424462e-06 reg 1.128838e+05 train accuracy: 0.254918 val accuracy: 0.249000\n",
      "lr 2.424462e-06 reg 1.832981e+05 train accuracy: 0.245449 val accuracy: 0.249000\n",
      "lr 2.424462e-06 reg 2.976351e+05 train accuracy: 0.218694 val accuracy: 0.216000\n",
      "lr 2.424462e-06 reg 4.832930e+05 train accuracy: 0.081633 val accuracy: 0.068000\n",
      "lr 2.424462e-06 reg 7.847600e+05 train accuracy: 0.094551 val accuracy: 0.103000\n",
      "lr 2.424462e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.424462e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 1.000000e+04 train accuracy: 0.350571 val accuracy: 0.356000\n",
      "lr 2.728333e-06 reg 1.623777e+04 train accuracy: 0.336878 val accuracy: 0.328000\n",
      "lr 2.728333e-06 reg 2.636651e+04 train accuracy: 0.321959 val accuracy: 0.327000\n",
      "lr 2.728333e-06 reg 4.281332e+04 train accuracy: 0.308429 val accuracy: 0.317000\n",
      "lr 2.728333e-06 reg 6.951928e+04 train accuracy: 0.257286 val accuracy: 0.262000\n",
      "lr 2.728333e-06 reg 1.128838e+05 train accuracy: 0.265020 val accuracy: 0.272000\n",
      "lr 2.728333e-06 reg 1.832981e+05 train accuracy: 0.251102 val accuracy: 0.279000\n",
      "lr 2.728333e-06 reg 2.976351e+05 train accuracy: 0.196878 val accuracy: 0.194000\n",
      "lr 2.728333e-06 reg 4.832930e+05 train accuracy: 0.077796 val accuracy: 0.052000\n",
      "lr 2.728333e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.728333e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 1.000000e+04 train accuracy: 0.351531 val accuracy: 0.358000\n",
      "lr 3.070291e-06 reg 1.623777e+04 train accuracy: 0.350592 val accuracy: 0.360000\n",
      "lr 3.070291e-06 reg 2.636651e+04 train accuracy: 0.333857 val accuracy: 0.335000\n",
      "lr 3.070291e-06 reg 4.281332e+04 train accuracy: 0.282143 val accuracy: 0.280000\n",
      "lr 3.070291e-06 reg 6.951928e+04 train accuracy: 0.252980 val accuracy: 0.268000\n",
      "lr 3.070291e-06 reg 1.128838e+05 train accuracy: 0.260531 val accuracy: 0.275000\n",
      "lr 3.070291e-06 reg 1.832981e+05 train accuracy: 0.211857 val accuracy: 0.223000\n",
      "lr 3.070291e-06 reg 2.976351e+05 train accuracy: 0.128184 val accuracy: 0.123000\n",
      "lr 3.070291e-06 reg 4.832930e+05 train accuracy: 0.144551 val accuracy: 0.147000\n",
      "lr 3.070291e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.070291e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 1.000000e+04 train accuracy: 0.340673 val accuracy: 0.370000\n",
      "lr 3.455107e-06 reg 1.623777e+04 train accuracy: 0.328102 val accuracy: 0.331000\n",
      "lr 3.455107e-06 reg 2.636651e+04 train accuracy: 0.287224 val accuracy: 0.294000\n",
      "lr 3.455107e-06 reg 4.281332e+04 train accuracy: 0.290592 val accuracy: 0.288000\n",
      "lr 3.455107e-06 reg 6.951928e+04 train accuracy: 0.269020 val accuracy: 0.290000\n",
      "lr 3.455107e-06 reg 1.128838e+05 train accuracy: 0.253327 val accuracy: 0.268000\n",
      "lr 3.455107e-06 reg 1.832981e+05 train accuracy: 0.206245 val accuracy: 0.222000\n",
      "lr 3.455107e-06 reg 2.976351e+05 train accuracy: 0.082449 val accuracy: 0.083000\n",
      "lr 3.455107e-06 reg 4.832930e+05 train accuracy: 0.098347 val accuracy: 0.110000\n",
      "lr 3.455107e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.455107e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 1.000000e+04 train accuracy: 0.337490 val accuracy: 0.343000\n",
      "lr 3.888155e-06 reg 1.623777e+04 train accuracy: 0.320714 val accuracy: 0.333000\n",
      "lr 3.888155e-06 reg 2.636651e+04 train accuracy: 0.295714 val accuracy: 0.321000\n",
      "lr 3.888155e-06 reg 4.281332e+04 train accuracy: 0.281510 val accuracy: 0.278000\n",
      "lr 3.888155e-06 reg 6.951928e+04 train accuracy: 0.273408 val accuracy: 0.280000\n",
      "lr 3.888155e-06 reg 1.128838e+05 train accuracy: 0.237714 val accuracy: 0.254000\n",
      "lr 3.888155e-06 reg 1.832981e+05 train accuracy: 0.119367 val accuracy: 0.120000\n",
      "lr 3.888155e-06 reg 2.976351e+05 train accuracy: 0.142367 val accuracy: 0.126000\n",
      "lr 3.888155e-06 reg 4.832930e+05 train accuracy: 0.092143 val accuracy: 0.089000\n",
      "lr 3.888155e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.888155e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 1.000000e+04 train accuracy: 0.304224 val accuracy: 0.316000\n",
      "lr 4.375479e-06 reg 1.623777e+04 train accuracy: 0.279653 val accuracy: 0.282000\n",
      "lr 4.375479e-06 reg 2.636651e+04 train accuracy: 0.309551 val accuracy: 0.313000\n",
      "lr 4.375479e-06 reg 4.281332e+04 train accuracy: 0.218163 val accuracy: 0.224000\n",
      "lr 4.375479e-06 reg 6.951928e+04 train accuracy: 0.263592 val accuracy: 0.264000\n",
      "lr 4.375479e-06 reg 1.128838e+05 train accuracy: 0.189286 val accuracy: 0.189000\n",
      "lr 4.375479e-06 reg 1.832981e+05 train accuracy: 0.108286 val accuracy: 0.108000\n",
      "lr 4.375479e-06 reg 2.976351e+05 train accuracy: 0.111490 val accuracy: 0.118000\n",
      "lr 4.375479e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.375479e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 1.000000e+04 train accuracy: 0.307204 val accuracy: 0.313000\n",
      "lr 4.923883e-06 reg 1.623777e+04 train accuracy: 0.289857 val accuracy: 0.289000\n",
      "lr 4.923883e-06 reg 2.636651e+04 train accuracy: 0.276327 val accuracy: 0.279000\n",
      "lr 4.923883e-06 reg 4.281332e+04 train accuracy: 0.263429 val accuracy: 0.276000\n",
      "lr 4.923883e-06 reg 6.951928e+04 train accuracy: 0.194551 val accuracy: 0.205000\n",
      "lr 4.923883e-06 reg 1.128838e+05 train accuracy: 0.136592 val accuracy: 0.134000\n",
      "lr 4.923883e-06 reg 1.832981e+05 train accuracy: 0.077857 val accuracy: 0.083000\n",
      "lr 4.923883e-06 reg 2.976351e+05 train accuracy: 0.110245 val accuracy: 0.113000\n",
      "lr 4.923883e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.923883e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 1.000000e+04 train accuracy: 0.259286 val accuracy: 0.281000\n",
      "lr 5.541020e-06 reg 1.623777e+04 train accuracy: 0.242000 val accuracy: 0.258000\n",
      "lr 5.541020e-06 reg 2.636651e+04 train accuracy: 0.216633 val accuracy: 0.221000\n",
      "lr 5.541020e-06 reg 4.281332e+04 train accuracy: 0.222163 val accuracy: 0.245000\n",
      "lr 5.541020e-06 reg 6.951928e+04 train accuracy: 0.140898 val accuracy: 0.140000\n",
      "lr 5.541020e-06 reg 1.128838e+05 train accuracy: 0.138000 val accuracy: 0.140000\n",
      "lr 5.541020e-06 reg 1.832981e+05 train accuracy: 0.126306 val accuracy: 0.117000\n",
      "lr 5.541020e-06 reg 2.976351e+05 train accuracy: 0.091653 val accuracy: 0.106000\n",
      "lr 5.541020e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.541020e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 1.000000e+04 train accuracy: 0.237673 val accuracy: 0.241000\n",
      "lr 6.235507e-06 reg 1.623777e+04 train accuracy: 0.240449 val accuracy: 0.241000\n",
      "lr 6.235507e-06 reg 2.636651e+04 train accuracy: 0.188837 val accuracy: 0.194000\n",
      "lr 6.235507e-06 reg 4.281332e+04 train accuracy: 0.185592 val accuracy: 0.193000\n",
      "lr 6.235507e-06 reg 6.951928e+04 train accuracy: 0.139592 val accuracy: 0.156000\n",
      "lr 6.235507e-06 reg 1.128838e+05 train accuracy: 0.092347 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 1.832981e+05 train accuracy: 0.115245 val accuracy: 0.139000\n",
      "lr 6.235507e-06 reg 2.976351e+05 train accuracy: 0.087980 val accuracy: 0.071000\n",
      "lr 6.235507e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 6.235507e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 1.000000e+04 train accuracy: 0.250061 val accuracy: 0.254000\n",
      "lr 7.017038e-06 reg 1.623777e+04 train accuracy: 0.199694 val accuracy: 0.211000\n",
      "lr 7.017038e-06 reg 2.636651e+04 train accuracy: 0.181959 val accuracy: 0.202000\n",
      "lr 7.017038e-06 reg 4.281332e+04 train accuracy: 0.157633 val accuracy: 0.159000\n",
      "lr 7.017038e-06 reg 6.951928e+04 train accuracy: 0.157122 val accuracy: 0.146000\n",
      "lr 7.017038e-06 reg 1.128838e+05 train accuracy: 0.110367 val accuracy: 0.108000\n",
      "lr 7.017038e-06 reg 1.832981e+05 train accuracy: 0.136163 val accuracy: 0.131000\n",
      "lr 7.017038e-06 reg 2.976351e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.017038e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 1.000000e+04 train accuracy: 0.247041 val accuracy: 0.240000\n",
      "lr 7.896523e-06 reg 1.623777e+04 train accuracy: 0.221735 val accuracy: 0.219000\n",
      "lr 7.896523e-06 reg 2.636651e+04 train accuracy: 0.177796 val accuracy: 0.178000\n",
      "lr 7.896523e-06 reg 4.281332e+04 train accuracy: 0.194755 val accuracy: 0.201000\n",
      "lr 7.896523e-06 reg 6.951928e+04 train accuracy: 0.172694 val accuracy: 0.166000\n",
      "lr 7.896523e-06 reg 1.128838e+05 train accuracy: 0.121653 val accuracy: 0.119000\n",
      "lr 7.896523e-06 reg 1.832981e+05 train accuracy: 0.098837 val accuracy: 0.106000\n",
      "lr 7.896523e-06 reg 2.976351e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.896523e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 1.000000e+04 train accuracy: 0.199776 val accuracy: 0.195000\n",
      "lr 8.886238e-06 reg 1.623777e+04 train accuracy: 0.164388 val accuracy: 0.176000\n",
      "lr 8.886238e-06 reg 2.636651e+04 train accuracy: 0.142184 val accuracy: 0.151000\n",
      "lr 8.886238e-06 reg 4.281332e+04 train accuracy: 0.178408 val accuracy: 0.177000\n",
      "lr 8.886238e-06 reg 6.951928e+04 train accuracy: 0.147347 val accuracy: 0.139000\n",
      "lr 8.886238e-06 reg 1.128838e+05 train accuracy: 0.085755 val accuracy: 0.078000\n",
      "lr 8.886238e-06 reg 1.832981e+05 train accuracy: 0.111837 val accuracy: 0.090000\n",
      "lr 8.886238e-06 reg 2.976351e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 8.886238e-06 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.212163 val accuracy: 0.227000\n",
      "lr 1.000000e-05 reg 1.623777e+04 train accuracy: 0.196939 val accuracy: 0.196000\n",
      "lr 1.000000e-05 reg 2.636651e+04 train accuracy: 0.178816 val accuracy: 0.188000\n",
      "lr 1.000000e-05 reg 4.281332e+04 train accuracy: 0.189429 val accuracy: 0.219000\n",
      "lr 1.000000e-05 reg 6.951928e+04 train accuracy: 0.100714 val accuracy: 0.100000\n",
      "lr 1.000000e-05 reg 1.128838e+05 train accuracy: 0.068061 val accuracy: 0.061000\n",
      "lr 1.000000e-05 reg 1.832981e+05 train accuracy: 0.104449 val accuracy: 0.120000\n",
      "lr 1.000000e-05 reg 2.976351e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 4.832930e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 7.847600e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.274275e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 2.069138e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 3.359818e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 5.455595e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 8.858668e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.438450e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 2.335721e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 3.792690e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 6.158482e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.387000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [5e4, 1e8]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "regularization_strengths   = np.logspace(4, 8, 20)\n",
    "learning_rates = np.logspace(-7, -6, 40)\n",
    "\n",
    "# loop regularization parameters\n",
    "for reg in regularization_strengths:\n",
    "\n",
    "    # loop learning rate parameters\n",
    "    for learn in learning_rates: \n",
    "\n",
    "        softmax        = Softmax()\n",
    "        loss_hist      = softmax.train(X_train, y_train, learning_rate=learn, reg=reg, num_iters=500, verbose=True)\n",
    "\n",
    "        y_train_pred   = softmax.predict(X_train)\n",
    "        y_val_pred     = softmax.predict(X_val)\n",
    "\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        val_accuracy   = np.mean(y_val == y_val_pred)\n",
    "\n",
    "        if (val_accuracy>best_val):\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = softmax\n",
    "\n",
    "        # store accuracy results\n",
    "        results[(learn, reg)] = (train_accuracy, val_accuracy)\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.370000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlydBEARBEIQY+G+vITK81lfzTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x77f58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
